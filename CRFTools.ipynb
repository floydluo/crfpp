{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare NLPText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/boson/bosonNER.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.660 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Num of All    Tokens 533491\n",
      "Total Num of Unique Tokens 3825\n",
      "CORPUS\tit is Dumped into file: data/boson/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tit is Dumped into file: data/boson/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/boson/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 1961\n",
      "SENT\tit is Dumped into file: data/boson/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 10214\n",
      "TOKEN\tit is Dumped into file: data/boson/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 533491\n",
      "**************************************** \n",
      "\n",
      "pos-bioes\tis Dumped into file: data/boson/char/Vocab/pos-bioes.voc\n",
      "pos-bioes\tthe length of it is   : 229\n",
      "\t\tWrite to: data/boson/char/Vocab/pos-bioes.tsv\n",
      "annoE-bioes\tis Dumped into file: data/boson/char/Vocab/annoE-bioes.voc\n",
      "annoE-bioes\tthe length of it is   : 25\n",
      "\t\tWrite to: data/boson/char/Vocab/annoE-bioes.tsv\n",
      "token   \tis Dumped into file: data/boson/char/Vocab/token.voc\n",
      "token   \tthe length of it is   : 3825\n",
      "\t\tWrite to: data/boson/char/Vocab/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "CORPUSPath = 'corpus/boson/'\n",
    "\n",
    "Corpus2GroupMethod = '.txt'\n",
    "\n",
    "Group2TextMethod   = 'line'\n",
    "\n",
    "Text2SentMethod  = 're'\n",
    "\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "\n",
    "min_token_freq = 1\n",
    "\n",
    "use_hyper = ['pos']\n",
    "\n",
    "anno = 'anno_embed_in_text'\n",
    "anno_keywords = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, \n",
    "                 Corpus2GroupMethod, \n",
    "                 Group2TextMethod, \n",
    "                 Text2SentMethod, \n",
    "                 Sent2TokenMethod, TOKENLevel, min_token_freq = min_token_freq,\n",
    "                 use_hyper = use_hyper, \n",
    "                 anno = anno, anno_keywords = anno_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n"
     ]
    }
   ],
   "source": [
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True,'Max_Ngram': 1,}, # always the char-level token\n",
    "    'basic':   {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'medical': {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'radical': {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'subcomp': {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'stroke':  {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    # CTX_DEP\n",
    "    'pos':     {'use': False, 'tagScheme': 'BIOE',},\n",
    "    # ANNO\n",
    "    'annoE':   {'use': False, 'tagScheme': 'BIOE',},\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE=CHANNEL_SETTINGS_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annoE',\n",
       " {'GU': (['</pad>',\n",
       "    'O',\n",
       "    'person_name-B',\n",
       "    'person_name-I',\n",
       "    'person_name-E',\n",
       "    'person_name-S',\n",
       "    'product_name-B',\n",
       "    'product_name-I',\n",
       "    'product_name-E',\n",
       "    'product_name-S',\n",
       "    'org_name-B',\n",
       "    'org_name-I',\n",
       "    'org_name-E',\n",
       "    'org_name-S',\n",
       "    'location-B',\n",
       "    'location-I',\n",
       "    'location-E',\n",
       "    'location-S',\n",
       "    'time-B',\n",
       "    'time-I',\n",
       "    'time-E',\n",
       "    'time-S',\n",
       "    'company_name-B',\n",
       "    'company_name-I',\n",
       "    'company_name-E',\n",
       "    'company_name-S'],\n",
       "   {0: '</pad>',\n",
       "    1: 'O',\n",
       "    2: 'person_name-B',\n",
       "    3: 'person_name-I',\n",
       "    4: 'person_name-E',\n",
       "    5: 'person_name-S',\n",
       "    6: 'product_name-B',\n",
       "    7: 'product_name-I',\n",
       "    8: 'product_name-E',\n",
       "    9: 'product_name-S',\n",
       "    10: 'org_name-B',\n",
       "    11: 'org_name-I',\n",
       "    12: 'org_name-E',\n",
       "    13: 'org_name-S',\n",
       "    14: 'location-B',\n",
       "    15: 'location-I',\n",
       "    16: 'location-E',\n",
       "    17: 'location-S',\n",
       "    18: 'time-B',\n",
       "    19: 'time-I',\n",
       "    20: 'time-E',\n",
       "    21: 'time-S',\n",
       "    22: 'company_name-B',\n",
       "    23: 'company_name-I',\n",
       "    24: 'company_name-E',\n",
       "    25: 'company_name-S'}),\n",
       "  'TRANS': {'0': 1,\n",
       "   '1': 2,\n",
       "   '2': 3,\n",
       "   '3': 4,\n",
       "   '4': 5,\n",
       "   '5': 6,\n",
       "   '6': 7,\n",
       "   '7': 8,\n",
       "   '8': 9,\n",
       "   '9': 10,\n",
       "   '10': 11,\n",
       "   '11': 12,\n",
       "   '12': 13,\n",
       "   '13': 14,\n",
       "   '14': 15,\n",
       "   '15': 16,\n",
       "   '16': 17,\n",
       "   '17': 18,\n",
       "   '18': 19,\n",
       "   '19': 20,\n",
       "   '20': 21,\n",
       "   '21': 22,\n",
       "   '22': 23,\n",
       "   '23': 24,\n",
       "   '24': 25}},\n",
       " ['company_name',\n",
       "  'location',\n",
       "  'org_name',\n",
       "  'person_name',\n",
       "  'product_name',\n",
       "  'time'],\n",
       " 26]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Target_Field = [\n",
    "    'annoE', {'tagScheme': 'BIOES'}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def get_target_field_info(nlptext, Target_Field, fldembed, useStartEnd = False):\n",
    "    field, para = Target_Field\n",
    "    # nlptext is important here.\n",
    "    # channel_anno = 'annoE'\n",
    "    target_para = {}\n",
    "    \n",
    "    if field == 'token':\n",
    "        # use start and end only works for token\n",
    "        special_num = 3 if useStartEnd else 1 \n",
    "        # from fldembed\n",
    "        Weights = fldembed.weights\n",
    "        assert 'token' in Weights\n",
    "        wv = Weights['token']\n",
    "        GU = wv.GU\n",
    "        idx2grain = ['</pad>'] + GU[0]\n",
    "        grain_num = len(idx2grain)\n",
    "        grain2idx = dict(zip(idx2grain, list(range(grain_num)) ))\n",
    "        GU = idx2grain, grain2idx\n",
    "        target_para['GU']  = GU # </pad> in GU, no </unk> in GU\n",
    "        labels = [] \n",
    "        tag_size = len(idx2grain) + special_num # </pad> in GU, and </unk>, </start>, </end> not in GU\n",
    "\n",
    "    else: \n",
    "        # generally, we won't adding start and end into CRF model.\n",
    "        # even the features are with start and end, it will should be removed before feed into crf\n",
    "        # it can also be proved that the start and end vectors are useless in sequential labeling.\n",
    "        GU = nlptext.getGrainVocab(field, **para)\n",
    "        idx2tag = ['</pad>'] + GU[0]\n",
    "        tag2idx = dict(zip(range(len(idx2tag)), idx2tag))\n",
    "        GU = idx2tag, tag2idx\n",
    "        # </pad> is in GU, no </unk> or other special tags in GU\n",
    "        tag_size = len(idx2tag)       \n",
    "        target_para['GU'] = GU\n",
    "        \n",
    "        TRANS = nlptext.getTrans(field, **para)\n",
    "        TRANS = {k: v+1 for k, v in TRANS.items()}\n",
    "        labels = list(set([i.split('-')[0] for i in idx2tag if '-' in i]))\n",
    "        labels.sort()\n",
    "        target_para['TRANS'] = TRANS\n",
    "        \n",
    "    # target_para['labels'] = labels\n",
    "    # target_para['tag_size'] = tag_size\n",
    "    \n",
    "    TARGET_FIELD = [field, target_para, labels, tag_size]\n",
    "    return TARGET_FIELD\n",
    "\n",
    "\n",
    "nlptext = BasicObject\n",
    "get_target_field_info(nlptext, Target_Field, None, useStartEnd = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "<st 12 (tokenNum: 155) >\n",
      "H M C 证 券 分 析 师 格 雷 格 ・ 罗 ( G r e g R o h ) 指 出 : “ 与 其 他 二 线 智 能 手 机 品 牌 相 比 , L G 手 机 的 质 量 有 了 一 定 的 提 升 , 但 在 品 牌 知 名 度 及 出 货 量 方 面 , L G 与 三 星 、 苹 果 等 手 机 巨 头 仍 有 不 小 的 差 距 。 ” L G 手 机 在 工 业 设 计 及 产 品 本 身 以 及 渠 道 方 面 都 与 三 星 存 在 着 一 定 的 差 距 , 从 软 件 到 硬 件 , 产 品 的 细 节 化 始 终 没 有 三 星 完 善 , 超 越 之 路 任 重 道 远 。\n",
      "{'token': {'Max_Ngram': 1}}\n"
     ]
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "sent = Sentence(12)\n",
    "print(sent.Idx)\n",
    "print(sent)\n",
    "print(sent.sentence)\n",
    "Channel_Settings = BasicObject.CHANNEL_SETTINGS\n",
    "print(Channel_Settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_sent_strfeats(sent, Channel_Settings, train = True):\n",
    "    '''\n",
    "        sent is a nlptex.sentence object\n",
    "        return a pandas dataframe\n",
    "    '''\n",
    "    try:\n",
    "        df = sent.feats\n",
    "        columns = df.columns\n",
    "        new_columns = [i for i in columns if i.split('_')[0] in Channel_Settings]\n",
    "        if not train:\n",
    "            new_columns = [i for i in new_columns if 'anno' not in i]\n",
    "        Feats = df.loc[new_columns]\n",
    "        return Feats\n",
    "    except:\n",
    "        features = {}\n",
    "        # stroke 12 and subcomp 6 are fixed internally.\n",
    "        for ch, cs in Channel_Settings.items():\n",
    "            if 'anno' in ch and not train:\n",
    "                continue\n",
    "            feature = sent.get_grain_str(ch)\n",
    "            # this will cost a lot of time\n",
    "            if ch == 'stroke':\n",
    "                max_leng = 12\n",
    "                feature2 = []\n",
    "                for token_feat in feature:\n",
    "                    if len(token_feat) <= max_leng:\n",
    "                        feature2.append(token_feat + ['</'] * (max_leng - len(token_feat))) \n",
    "                    else:\n",
    "                        feature2.append(token_feat[:max_leng])\n",
    "                feature = feature2\n",
    "            elif ch == 'subcomp':\n",
    "                max_leng = 6\n",
    "                feature2 = []\n",
    "                for token_feat in feature:\n",
    "                    if len(token_feat) <= max_leng:\n",
    "                        feature2.append(token_feat + ['</'] * (max_leng - len(token_feat)))\n",
    "                    else:\n",
    "                        feature2.append(token_feat[:max_leng])\n",
    "                feature = feature2 \n",
    "            features[ch] = feature\n",
    "            # print(feature)\n",
    "        L = []\n",
    "        for ch, feat in features.items():\n",
    "            df = pd.DataFrame(feat)\n",
    "            df.columns = [ch + '_' + str(i) for i in range(df.shape[1])]\n",
    "            L.append(df)\n",
    "        \n",
    "        Feats = pd.concat(L, axis = 1)\n",
    "        return Feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>证</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>券</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>分</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>析</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>师</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>格</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>雷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>格</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>・</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>罗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>指</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>出</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>与</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>其</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>他</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>二</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>软</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>件</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>到</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>硬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>件</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>产</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>细</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>节</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>化</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>始</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>终</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>没</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>有</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>三</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>星</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>完</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>善</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>超</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>越</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>之</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>路</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>任</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>重</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>远</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_0\n",
       "0         H\n",
       "1         M\n",
       "2         C\n",
       "3         证\n",
       "4         券\n",
       "5         分\n",
       "6         析\n",
       "7         师\n",
       "8         格\n",
       "9         雷\n",
       "10        格\n",
       "11        ・\n",
       "12        罗\n",
       "13        (\n",
       "14        G\n",
       "15        r\n",
       "16        e\n",
       "17        g\n",
       "18        R\n",
       "19        o\n",
       "20        h\n",
       "21        )\n",
       "22        指\n",
       "23        出\n",
       "24        :\n",
       "25        “\n",
       "26        与\n",
       "27        其\n",
       "28        他\n",
       "29        二\n",
       "..      ...\n",
       "125       软\n",
       "126       件\n",
       "127       到\n",
       "128       硬\n",
       "129       件\n",
       "130       ,\n",
       "131       产\n",
       "132       品\n",
       "133       的\n",
       "134       细\n",
       "135       节\n",
       "136       化\n",
       "137       始\n",
       "138       终\n",
       "139       没\n",
       "140       有\n",
       "141       三\n",
       "142       星\n",
       "143       完\n",
       "144       善\n",
       "145       ,\n",
       "146       超\n",
       "147       越\n",
       "148       之\n",
       "149       路\n",
       "150       任\n",
       "151       重\n",
       "152       道\n",
       "153       远\n",
       "154       。\n",
       "\n",
       "[155 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from crfpp.crftools import get_sent_strfeats\n",
    "get_sent_strfeats(sent, Channel_Settings) # this need some attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Features (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fieldembed import FieldEmbedding\n",
    "# fieldembed = FieldEmbedding.load('embeddings/fieldembed/Ch_wiki_char_subcomp_model')\n",
    "# fieldembed.Field_Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv_subcomp = fieldembed.weights['subcomp']# .GU\n",
    "# wv_subcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv_subcomp.derivative_wv.GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_vecfeats(sent, fieldembed, train = True):\n",
    "    features = {}\n",
    "    Channel_Settings = fieldembed.Field_Settings\n",
    "    token_strs = [i[0] for i in sent.get_grain_str('token')]\n",
    "    # print(token_strs)\n",
    "    features['origin'] = token_strs\n",
    "    for ch, cs in Channel_Settings.items():\n",
    "        if 'anno' in ch and not train:\n",
    "            continue\n",
    "        derivative_wv = fieldembed.weights[ch].derivative_wv\n",
    "        TU = derivative_wv.GU # LGU in derivative wv is LTU\n",
    "        # this code is verbose\n",
    "        token_idxes = [TU[1].get(token_str, 0) for token_str in token_strs] # 0 is not unk, to fix it in the future\n",
    "        # token_idxes = [i[0] for i in token_idxes]\n",
    "        # print(token_idxes)\n",
    "        feature = derivative_wv.vectors[token_idxes]\n",
    "        features[ch] = feature\n",
    "        # print(feature)\n",
    "    L = []\n",
    "    for ch, feat in features.items():\n",
    "        df = pd.DataFrame(feat)\n",
    "        df.columns = [ch + '_' + str(i) for i in range(df.shape[1])]\n",
    "        L.append(df)\n",
    "\n",
    "    Feats = pd.concat(L, axis = 1)\n",
    "    return Feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # from crfpp.crftools import get_sent_vecfeats\n",
    "\n",
    "# get_sent_vecfeats(sent, fieldembed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize all Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlptext.base.BasicObject"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def featurize_nlptext_sentences(BasicObject, feat_type = 'str', fieldembed = None):\n",
    "    Total_Settings = {\n",
    "        'token': {'Max_Ngram': 1},\n",
    "        'char': {'Max_Ngram': 1, 'end_grain': False},\n",
    "        'basic': {'Max_Ngram': 1, 'end_grain': False},\n",
    "        'radical': {'Max_Ngram': 1, 'end_grain': False},\n",
    "        'subcomp': {'Max_Ngram': 1, 'end_grain': False},\n",
    "        'stroke': {'Max_Ngram': 1, 'end_grain': False},\n",
    "        'pos': {'Max_Ngram': 1, 'end_grain': False,   'tagScheme': 'BIOE'},\n",
    "        'annoE': {'Max_Ngram': 1, 'end_grain': False, 'tagScheme': 'BIOE'}\n",
    "    }\n",
    "\n",
    "    # prepare path to save the features.\n",
    "    if 'str' in feat_type.lower():\n",
    "        sentence_path = BasicObject.Data_Dir + '/' + 'Pyramid/_Feat_SENT_Str.crfpp'\n",
    "        get_sent_feats = get_sent_strfeats\n",
    "    elif 'vec' in feat_type.lower():\n",
    "        sentence_path = BasicObject.Data_Dir + '/' + 'Pyramid/_Feat_SENT_Vec.crfpp'\n",
    "        get_sent_feats = get_sent_vecfeats\n",
    "    else:\n",
    "        print('Error! Currently there is no such a feature type')\n",
    "        \n",
    "    # if there is \n",
    "    if os.path.isfile(sentence_path):\n",
    "\n",
    "        with open(sentence_path, 'rb') as handle:\n",
    "            sents = pickle.load(handle)\n",
    "        print('Load Featurized Sentences from:')\n",
    "        print('\\t', sentence_path)\n",
    "        return sents\n",
    "    else:\n",
    "        from nlptext.sentence import Sentence\n",
    "        # corpus = Corpus() # this costs time\n",
    "        sents = []\n",
    "        if fieldembed:\n",
    "            Total_Settings = fieldembed\n",
    "        for sentidx in range(BasicObject.SENT['length']):\n",
    "            sent = Sentence(sentidx)\n",
    "            sent.feats = get_sent_feats(sent, Total_Settings)\n",
    "            sents.append(sent)\n",
    "        with open(sentence_path, 'wb') as handle:\n",
    "            pickle.dump(sents, handle)\n",
    "        print('Save Featurized Sentences to:')\n",
    "        print('\\t', sentence_path)\n",
    "        return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize all Sentences with String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBuild GrainUnique for channel: pos-bio\n",
      "pos 1 False BIO\n",
      "\t\tWrite to: data/boson/char/Vocab/pos-bio.voc\n",
      "\t\tWrite to: data/boson/char/Vocab/pos-bio.tsv\n",
      "\t\tBuild GrainUnique for channel: annoE-bio\n",
      "annoE 1 False BIO\n",
      "\t\tWrite to: data/boson/char/Vocab/annoE-bio.voc\n",
      "\t\tWrite to: data/boson/char/Vocab/annoE-bio.tsv\n",
      "Save Featurized Sentences to:\n",
      "\t data/boson/char/Pyramid/_Feat_SENT_Str.crfpp\n"
     ]
    }
   ],
   "source": [
    "sents = featurize_nlptext_sentences(BasicObject, feat_type = 'str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize all Sentences with Vector (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fieldembed import FieldEmbedding\n",
    "# fieldembed = FieldEmbedding.load('embeddings/fieldembed/Ch_wiki_char_subcomp_model')\n",
    "# fieldembed.Field_Settings\n",
    "\n",
    "# sents = featurize_nlptext_sentences(BasicObject, feat_type = 'vec', fieldembed=fieldembed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Template\n",
    "\n",
    "The `template` is used to convert `input feats` to `derivative feats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unigram\n",
      "\n",
      "U0:%x[0,0]\n",
      "U1:%x[0,1]\n",
      "U2:%x[0,2]\n",
      "U3:%x[0,3]\n",
      "U4:%x[0,4]\n",
      "U5:%x[0,5]\n",
      "U6:%x[0,6]\n",
      "U7:%x[0,7]\n",
      "U8:%x[0,8]\n",
      "U9:%x[0,9]\n",
      "U10:%x[0,10]\n",
      "U11:%x[0,11]\n",
      "U12:%x[0,12]\n",
      "U13:%x[0,13]\n",
      "U14:%x[0,14]\n",
      "U15:%x[0,15]\n",
      "U16:%x[0,16]\n",
      "U17:%x[0,17]\n",
      "U18:%x[0,18]\n",
      "U19:%x[0,19]\n",
      "\n",
      "\n",
      "# Bigram\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# individaul\n",
    "\n",
    "individual_para = {\n",
    "    1: 0,\n",
    "}\n",
    "\n",
    "def generate_template(input_feats_num = 5, individual_para = individual_para, path = None):\n",
    "    '''\n",
    "     this still needs more consideration\n",
    "    '''\n",
    "    if not path:\n",
    "        path = '_template'\n",
    "        \n",
    "    L = ['# Unigram\\n\\n']\n",
    "    fld_idx = 0\n",
    "    for feat_idx in range(input_feats_num):\n",
    "        for gram_num, window_size in individual_para.items():\n",
    "            if gram_num == 1:\n",
    "                for token_i in range(-window_size, window_size + 1):\n",
    "                    L.append('U{}:%x[{},{}]\\n'.format(str(fld_idx), str(token_i), str(feat_idx)))\n",
    "                    fld_idx = fld_idx + 1\n",
    "            if gram_num == 2 and feat_idx <= 5:\n",
    "                for token_i in range(-window_size, window_size + 1):\n",
    "                    L.append('U{}:%x[{},{}]/%x[{},{}]\\n'.format(str(fld_idx), \n",
    "                                                                str(token_i), str(feat_idx), \n",
    "                                                                str(token_i + 1), str(feat_idx)))\n",
    "                    fld_idx = fld_idx + 1\n",
    "                    \n",
    "            if gram_num == 3 and feat_idx <= 5:\n",
    "                for token_i in range(-window_size, window_size + 1):\n",
    "                    L.append('U{}:%x[{},{}]/%x[{},{}]/%x[{},{}]\\n'.format(str(fld_idx), \n",
    "                                                                          str(token_i - 1), str(feat_idx), \n",
    "                                                                          str(token_i), str(feat_idx), \n",
    "                                                                          str(token_i + 1), str(feat_idx)))\n",
    "                    fld_idx = fld_idx + 1\n",
    "    \n",
    "    \n",
    "    L.append('\\n\\n# Bigram\\nB')\n",
    "    \n",
    "    with open(path, 'w') as f:\n",
    "        f.write(''.join(L))\n",
    "        \n",
    "    return ''.join(L)\n",
    "\n",
    "print(generate_template(input_feats_num = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<st 0 (tokenNum: 40) >,\n",
       " <st 1 (tokenNum: 23) >,\n",
       " <st 2 (tokenNum: 75) >,\n",
       " <st 3 (tokenNum: 28) >,\n",
       " <st 4 (tokenNum: 57) >,\n",
       " <st 5 (tokenNum: 17) >,\n",
       " <st 6 (tokenNum: 41) >,\n",
       " <st 7 (tokenNum: 112) >,\n",
       " <st 8 (tokenNum: 65) >,\n",
       " <st 9 (tokenNum: 64) >,\n",
       " <st 10 (tokenNum: 66) >,\n",
       " <st 11 (tokenNum: 90) >,\n",
       " <st 12 (tokenNum: 155) >,\n",
       " <st 13 (tokenNum: 113) >,\n",
       " <st 14 (tokenNum: 63) >,\n",
       " <st 15 (tokenNum: 59) >,\n",
       " <st 16 (tokenNum: 68) >,\n",
       " <st 17 (tokenNum: 28) >,\n",
       " <st 18 (tokenNum: 38) >,\n",
       " <st 19 (tokenNum: 13) >,\n",
       " <st 20 (tokenNum: 146) >,\n",
       " <st 21 (tokenNum: 39) >,\n",
       " <st 22 (tokenNum: 30) >,\n",
       " <st 23 (tokenNum: 20) >,\n",
       " <st 24 (tokenNum: 55) >,\n",
       " <st 25 (tokenNum: 29) >,\n",
       " <st 26 (tokenNum: 15) >,\n",
       " <st 27 (tokenNum: 11) >,\n",
       " <st 28 (tokenNum: 60) >,\n",
       " <st 29 (tokenNum: 43) >,\n",
       " <st 30 (tokenNum: 117) >,\n",
       " <st 31 (tokenNum: 25) >,\n",
       " <st 32 (tokenNum: 125) >,\n",
       " <st 33 (tokenNum: 28) >,\n",
       " <st 34 (tokenNum: 45) >,\n",
       " <st 35 (tokenNum: 88) >,\n",
       " <st 36 (tokenNum: 21) >,\n",
       " <st 37 (tokenNum: 145) >,\n",
       " <st 38 (tokenNum: 6) >,\n",
       " <st 39 (tokenNum: 118) >,\n",
       " <st 40 (tokenNum: 51) >,\n",
       " <st 41 (tokenNum: 52) >,\n",
       " <st 42 (tokenNum: 54) >,\n",
       " <st 43 (tokenNum: 55) >,\n",
       " <st 44 (tokenNum: 58) >,\n",
       " <st 45 (tokenNum: 77) >,\n",
       " <st 46 (tokenNum: 28) >,\n",
       " <st 47 (tokenNum: 25) >,\n",
       " <st 48 (tokenNum: 21) >,\n",
       " <st 49 (tokenNum: 72) >,\n",
       " <st 50 (tokenNum: 71) >,\n",
       " <st 51 (tokenNum: 23) >,\n",
       " <st 52 (tokenNum: 34) >,\n",
       " <st 53 (tokenNum: 54) >,\n",
       " <st 54 (tokenNum: 139) >,\n",
       " <st 55 (tokenNum: 49) >,\n",
       " <st 56 (tokenNum: 41) >,\n",
       " <st 57 (tokenNum: 43) >,\n",
       " <st 58 (tokenNum: 28) >,\n",
       " <st 59 (tokenNum: 32) >,\n",
       " <st 60 (tokenNum: 119) >,\n",
       " <st 61 (tokenNum: 84) >,\n",
       " <st 62 (tokenNum: 90) >,\n",
       " <st 63 (tokenNum: 45) >,\n",
       " <st 64 (tokenNum: 14) >,\n",
       " <st 65 (tokenNum: 34) >,\n",
       " <st 66 (tokenNum: 69) >,\n",
       " <st 67 (tokenNum: 41) >,\n",
       " <st 68 (tokenNum: 10) >,\n",
       " <st 69 (tokenNum: 71) >,\n",
       " <st 70 (tokenNum: 39) >,\n",
       " <st 71 (tokenNum: 47) >,\n",
       " <st 72 (tokenNum: 36) >,\n",
       " <st 73 (tokenNum: 54) >,\n",
       " <st 74 (tokenNum: 87) >,\n",
       " <st 75 (tokenNum: 65) >,\n",
       " <st 76 (tokenNum: 106) >,\n",
       " <st 77 (tokenNum: 18) >,\n",
       " <st 78 (tokenNum: 45) >,\n",
       " <st 79 (tokenNum: 66) >,\n",
       " <st 80 (tokenNum: 39) >,\n",
       " <st 81 (tokenNum: 66) >,\n",
       " <st 82 (tokenNum: 86) >,\n",
       " <st 83 (tokenNum: 38) >,\n",
       " <st 84 (tokenNum: 81) >,\n",
       " <st 85 (tokenNum: 120) >,\n",
       " <st 86 (tokenNum: 289) >,\n",
       " <st 87 (tokenNum: 44) >,\n",
       " <st 88 (tokenNum: 44) >,\n",
       " <st 89 (tokenNum: 41) >,\n",
       " <st 90 (tokenNum: 46) >,\n",
       " <st 91 (tokenNum: 39) >,\n",
       " <st 92 (tokenNum: 82) >,\n",
       " <st 93 (tokenNum: 85) >,\n",
       " <st 94 (tokenNum: 25) >,\n",
       " <st 95 (tokenNum: 27) >,\n",
       " <st 96 (tokenNum: 88) >,\n",
       " <st 97 (tokenNum: 80) >,\n",
       " <st 98 (tokenNum: 49) >,\n",
       " <st 99 (tokenNum: 59) >,\n",
       " <st 100 (tokenNum: 66) >,\n",
       " <st 101 (tokenNum: 88) >,\n",
       " <st 102 (tokenNum: 47) >,\n",
       " <st 103 (tokenNum: 44) >,\n",
       " <st 104 (tokenNum: 39) >,\n",
       " <st 105 (tokenNum: 35) >,\n",
       " <st 106 (tokenNum: 30) >,\n",
       " <st 107 (tokenNum: 36) >,\n",
       " <st 108 (tokenNum: 82) >,\n",
       " <st 109 (tokenNum: 41) >,\n",
       " <st 110 (tokenNum: 37) >,\n",
       " <st 111 (tokenNum: 23) >,\n",
       " <st 112 (tokenNum: 40) >,\n",
       " <st 113 (tokenNum: 84) >,\n",
       " <st 114 (tokenNum: 40) >,\n",
       " <st 115 (tokenNum: 65) >,\n",
       " <st 116 (tokenNum: 98) >,\n",
       " <st 117 (tokenNum: 44) >,\n",
       " <st 118 (tokenNum: 59) >,\n",
       " <st 119 (tokenNum: 23) >,\n",
       " <st 120 (tokenNum: 6) >,\n",
       " <st 121 (tokenNum: 86) >,\n",
       " <st 122 (tokenNum: 15) >,\n",
       " <st 123 (tokenNum: 114) >,\n",
       " <st 124 (tokenNum: 61) >,\n",
       " <st 125 (tokenNum: 36) >,\n",
       " <st 126 (tokenNum: 14) >,\n",
       " <st 127 (tokenNum: 17) >,\n",
       " <st 128 (tokenNum: 89) >,\n",
       " <st 129 (tokenNum: 24) >,\n",
       " <st 130 (tokenNum: 49) >,\n",
       " <st 131 (tokenNum: 11) >,\n",
       " <st 132 (tokenNum: 98) >,\n",
       " <st 133 (tokenNum: 33) >,\n",
       " <st 134 (tokenNum: 57) >,\n",
       " <st 135 (tokenNum: 62) >,\n",
       " <st 136 (tokenNum: 47) >,\n",
       " <st 137 (tokenNum: 19) >,\n",
       " <st 138 (tokenNum: 6) >,\n",
       " <st 139 (tokenNum: 6) >,\n",
       " <st 140 (tokenNum: 79) >,\n",
       " <st 141 (tokenNum: 41) >,\n",
       " <st 142 (tokenNum: 79) >,\n",
       " <st 143 (tokenNum: 69) >,\n",
       " <st 144 (tokenNum: 38) >,\n",
       " <st 145 (tokenNum: 40) >,\n",
       " <st 146 (tokenNum: 46) >,\n",
       " <st 147 (tokenNum: 117) >,\n",
       " <st 148 (tokenNum: 47) >,\n",
       " <st 149 (tokenNum: 51) >,\n",
       " <st 150 (tokenNum: 46) >,\n",
       " <st 151 (tokenNum: 73) >,\n",
       " <st 152 (tokenNum: 95) >,\n",
       " <st 153 (tokenNum: 99) >,\n",
       " <st 154 (tokenNum: 28) >,\n",
       " <st 155 (tokenNum: 44) >,\n",
       " <st 156 (tokenNum: 67) >,\n",
       " <st 157 (tokenNum: 82) >,\n",
       " <st 158 (tokenNum: 57) >,\n",
       " <st 159 (tokenNum: 41) >,\n",
       " <st 160 (tokenNum: 63) >,\n",
       " <st 161 (tokenNum: 38) >,\n",
       " <st 162 (tokenNum: 28) >,\n",
       " <st 163 (tokenNum: 58) >,\n",
       " <st 164 (tokenNum: 61) >,\n",
       " <st 165 (tokenNum: 48) >,\n",
       " <st 166 (tokenNum: 49) >,\n",
       " <st 167 (tokenNum: 75) >,\n",
       " <st 168 (tokenNum: 16) >,\n",
       " <st 169 (tokenNum: 36) >,\n",
       " <st 170 (tokenNum: 31) >,\n",
       " <st 171 (tokenNum: 28) >,\n",
       " <st 172 (tokenNum: 51) >,\n",
       " <st 173 (tokenNum: 22) >,\n",
       " <st 174 (tokenNum: 13) >,\n",
       " <st 175 (tokenNum: 22) >,\n",
       " <st 176 (tokenNum: 31) >,\n",
       " <st 177 (tokenNum: 92) >,\n",
       " <st 178 (tokenNum: 34) >,\n",
       " <st 179 (tokenNum: 60) >,\n",
       " <st 180 (tokenNum: 74) >,\n",
       " <st 181 (tokenNum: 58) >,\n",
       " <st 182 (tokenNum: 60) >,\n",
       " <st 183 (tokenNum: 20) >,\n",
       " <st 184 (tokenNum: 37) >,\n",
       " <st 185 (tokenNum: 57) >,\n",
       " <st 186 (tokenNum: 23) >,\n",
       " <st 187 (tokenNum: 46) >,\n",
       " <st 188 (tokenNum: 37) >,\n",
       " <st 189 (tokenNum: 83) >,\n",
       " <st 190 (tokenNum: 48) >,\n",
       " <st 191 (tokenNum: 46) >,\n",
       " <st 192 (tokenNum: 45) >,\n",
       " <st 193 (tokenNum: 27) >,\n",
       " <st 194 (tokenNum: 39) >,\n",
       " <st 195 (tokenNum: 32) >,\n",
       " <st 196 (tokenNum: 54) >,\n",
       " <st 197 (tokenNum: 61) >,\n",
       " <st 198 (tokenNum: 28) >,\n",
       " <st 199 (tokenNum: 59) >,\n",
       " <st 200 (tokenNum: 64) >,\n",
       " <st 201 (tokenNum: 92) >,\n",
       " <st 202 (tokenNum: 38) >,\n",
       " <st 203 (tokenNum: 91) >,\n",
       " <st 204 (tokenNum: 44) >,\n",
       " <st 205 (tokenNum: 76) >,\n",
       " <st 206 (tokenNum: 26) >,\n",
       " <st 207 (tokenNum: 78) >,\n",
       " <st 208 (tokenNum: 33) >,\n",
       " <st 209 (tokenNum: 18) >,\n",
       " <st 210 (tokenNum: 17) >,\n",
       " <st 211 (tokenNum: 65) >,\n",
       " <st 212 (tokenNum: 28) >,\n",
       " <st 213 (tokenNum: 114) >,\n",
       " <st 214 (tokenNum: 124) >,\n",
       " <st 215 (tokenNum: 99) >,\n",
       " <st 216 (tokenNum: 18) >,\n",
       " <st 217 (tokenNum: 44) >,\n",
       " <st 218 (tokenNum: 83) >,\n",
       " <st 219 (tokenNum: 23) >,\n",
       " <st 220 (tokenNum: 114) >,\n",
       " <st 221 (tokenNum: 124) >,\n",
       " <st 222 (tokenNum: 99) >,\n",
       " <st 223 (tokenNum: 18) >,\n",
       " <st 224 (tokenNum: 44) >,\n",
       " <st 225 (tokenNum: 83) >,\n",
       " <st 226 (tokenNum: 23) >,\n",
       " <st 227 (tokenNum: 136) >,\n",
       " <st 228 (tokenNum: 33) >,\n",
       " <st 229 (tokenNum: 111) >,\n",
       " <st 230 (tokenNum: 74) >,\n",
       " <st 231 (tokenNum: 93) >,\n",
       " <st 232 (tokenNum: 26) >,\n",
       " <st 233 (tokenNum: 35) >,\n",
       " <st 234 (tokenNum: 22) >,\n",
       " <st 235 (tokenNum: 45) >,\n",
       " <st 236 (tokenNum: 80) >,\n",
       " <st 237 (tokenNum: 29) >,\n",
       " <st 238 (tokenNum: 90) >,\n",
       " <st 239 (tokenNum: 65) >,\n",
       " <st 240 (tokenNum: 17) >,\n",
       " <st 241 (tokenNum: 32) >,\n",
       " <st 242 (tokenNum: 72) >,\n",
       " <st 243 (tokenNum: 67) >,\n",
       " <st 244 (tokenNum: 19) >,\n",
       " <st 245 (tokenNum: 13) >,\n",
       " <st 246 (tokenNum: 110) >,\n",
       " <st 247 (tokenNum: 91) >,\n",
       " <st 248 (tokenNum: 13) >,\n",
       " <st 249 (tokenNum: 44) >,\n",
       " <st 250 (tokenNum: 40) >,\n",
       " <st 251 (tokenNum: 80) >,\n",
       " <st 252 (tokenNum: 28) >,\n",
       " <st 253 (tokenNum: 100) >,\n",
       " <st 254 (tokenNum: 40) >,\n",
       " <st 255 (tokenNum: 36) >,\n",
       " <st 256 (tokenNum: 35) >,\n",
       " <st 257 (tokenNum: 41) >,\n",
       " <st 258 (tokenNum: 96) >,\n",
       " <st 259 (tokenNum: 25) >,\n",
       " <st 260 (tokenNum: 59) >,\n",
       " <st 261 (tokenNum: 32) >,\n",
       " <st 262 (tokenNum: 34) >,\n",
       " <st 263 (tokenNum: 52) >,\n",
       " <st 264 (tokenNum: 48) >,\n",
       " <st 265 (tokenNum: 107) >,\n",
       " <st 266 (tokenNum: 51) >,\n",
       " <st 267 (tokenNum: 75) >,\n",
       " <st 268 (tokenNum: 79) >,\n",
       " <st 269 (tokenNum: 30) >,\n",
       " <st 270 (tokenNum: 23) >,\n",
       " <st 271 (tokenNum: 116) >,\n",
       " <st 272 (tokenNum: 39) >,\n",
       " <st 273 (tokenNum: 28) >,\n",
       " <st 274 (tokenNum: 42) >,\n",
       " <st 275 (tokenNum: 79) >,\n",
       " <st 276 (tokenNum: 30) >,\n",
       " <st 277 (tokenNum: 23) >,\n",
       " <st 278 (tokenNum: 116) >,\n",
       " <st 279 (tokenNum: 39) >,\n",
       " <st 280 (tokenNum: 28) >,\n",
       " <st 281 (tokenNum: 42) >,\n",
       " <st 282 (tokenNum: 28) >,\n",
       " <st 283 (tokenNum: 59) >,\n",
       " <st 284 (tokenNum: 52) >,\n",
       " <st 285 (tokenNum: 69) >,\n",
       " <st 286 (tokenNum: 17) >,\n",
       " <st 287 (tokenNum: 85) >,\n",
       " <st 288 (tokenNum: 38) >,\n",
       " <st 289 (tokenNum: 43) >,\n",
       " <st 290 (tokenNum: 113) >,\n",
       " <st 291 (tokenNum: 42) >,\n",
       " <st 292 (tokenNum: 43) >,\n",
       " <st 293 (tokenNum: 106) >,\n",
       " <st 294 (tokenNum: 54) >,\n",
       " <st 295 (tokenNum: 41) >,\n",
       " <st 296 (tokenNum: 53) >,\n",
       " <st 297 (tokenNum: 46) >,\n",
       " <st 298 (tokenNum: 52) >,\n",
       " <st 299 (tokenNum: 51) >,\n",
       " <st 300 (tokenNum: 39) >,\n",
       " <st 301 (tokenNum: 38) >,\n",
       " <st 302 (tokenNum: 22) >,\n",
       " <st 303 (tokenNum: 134) >,\n",
       " <st 304 (tokenNum: 38) >,\n",
       " <st 305 (tokenNum: 8) >,\n",
       " <st 306 (tokenNum: 54) >,\n",
       " <st 307 (tokenNum: 9) >,\n",
       " <st 308 (tokenNum: 142) >,\n",
       " <st 309 (tokenNum: 14) >,\n",
       " <st 310 (tokenNum: 20) >,\n",
       " <st 311 (tokenNum: 33) >,\n",
       " <st 312 (tokenNum: 56) >,\n",
       " <st 313 (tokenNum: 89) >,\n",
       " <st 314 (tokenNum: 23) >,\n",
       " <st 315 (tokenNum: 48) >,\n",
       " <st 316 (tokenNum: 16) >,\n",
       " <st 317 (tokenNum: 51) >,\n",
       " <st 318 (tokenNum: 56) >,\n",
       " <st 319 (tokenNum: 52) >,\n",
       " <st 320 (tokenNum: 28) >,\n",
       " <st 321 (tokenNum: 46) >,\n",
       " <st 322 (tokenNum: 49) >,\n",
       " <st 323 (tokenNum: 39) >,\n",
       " <st 324 (tokenNum: 30) >,\n",
       " <st 325 (tokenNum: 32) >,\n",
       " <st 326 (tokenNum: 65) >,\n",
       " <st 327 (tokenNum: 78) >,\n",
       " <st 328 (tokenNum: 70) >,\n",
       " <st 329 (tokenNum: 41) >,\n",
       " <st 330 (tokenNum: 86) >,\n",
       " <st 331 (tokenNum: 70) >,\n",
       " <st 332 (tokenNum: 88) >,\n",
       " <st 333 (tokenNum: 93) >,\n",
       " <st 334 (tokenNum: 64) >,\n",
       " <st 335 (tokenNum: 104) >,\n",
       " <st 336 (tokenNum: 61) >,\n",
       " <st 337 (tokenNum: 53) >,\n",
       " <st 338 (tokenNum: 11) >,\n",
       " <st 339 (tokenNum: 47) >,\n",
       " <st 340 (tokenNum: 35) >,\n",
       " <st 341 (tokenNum: 35) >,\n",
       " <st 342 (tokenNum: 24) >,\n",
       " <st 343 (tokenNum: 44) >,\n",
       " <st 344 (tokenNum: 41) >,\n",
       " <st 345 (tokenNum: 29) >,\n",
       " <st 346 (tokenNum: 24) >,\n",
       " <st 347 (tokenNum: 29) >,\n",
       " <st 348 (tokenNum: 36) >,\n",
       " <st 349 (tokenNum: 32) >,\n",
       " <st 350 (tokenNum: 55) >,\n",
       " <st 351 (tokenNum: 30) >,\n",
       " <st 352 (tokenNum: 57) >,\n",
       " <st 353 (tokenNum: 23) >,\n",
       " <st 354 (tokenNum: 34) >,\n",
       " <st 355 (tokenNum: 40) >,\n",
       " <st 356 (tokenNum: 82) >,\n",
       " <st 357 (tokenNum: 57) >,\n",
       " <st 358 (tokenNum: 21) >,\n",
       " <st 359 (tokenNum: 44) >,\n",
       " <st 360 (tokenNum: 53) >,\n",
       " <st 361 (tokenNum: 37) >,\n",
       " <st 362 (tokenNum: 15) >,\n",
       " <st 363 (tokenNum: 48) >,\n",
       " <st 364 (tokenNum: 18) >,\n",
       " <st 365 (tokenNum: 30) >,\n",
       " <st 366 (tokenNum: 38) >,\n",
       " <st 367 (tokenNum: 28) >,\n",
       " <st 368 (tokenNum: 35) >,\n",
       " <st 369 (tokenNum: 101) >,\n",
       " <st 370 (tokenNum: 93) >,\n",
       " <st 371 (tokenNum: 106) >,\n",
       " <st 372 (tokenNum: 81) >,\n",
       " <st 373 (tokenNum: 37) >,\n",
       " <st 374 (tokenNum: 25) >,\n",
       " <st 375 (tokenNum: 56) >,\n",
       " <st 376 (tokenNum: 18) >,\n",
       " <st 377 (tokenNum: 89) >,\n",
       " <st 378 (tokenNum: 35) >,\n",
       " <st 379 (tokenNum: 30) >,\n",
       " <st 380 (tokenNum: 42) >,\n",
       " <st 381 (tokenNum: 35) >,\n",
       " <st 382 (tokenNum: 125) >,\n",
       " <st 383 (tokenNum: 68) >,\n",
       " <st 384 (tokenNum: 108) >,\n",
       " <st 385 (tokenNum: 38) >,\n",
       " <st 386 (tokenNum: 39) >,\n",
       " <st 387 (tokenNum: 53) >,\n",
       " <st 388 (tokenNum: 119) >,\n",
       " <st 389 (tokenNum: 33) >,\n",
       " <st 390 (tokenNum: 51) >,\n",
       " <st 391 (tokenNum: 43) >,\n",
       " <st 392 (tokenNum: 22) >,\n",
       " <st 393 (tokenNum: 20) >,\n",
       " <st 394 (tokenNum: 27) >,\n",
       " <st 395 (tokenNum: 22) >,\n",
       " <st 396 (tokenNum: 22) >,\n",
       " <st 397 (tokenNum: 27) >,\n",
       " <st 398 (tokenNum: 18) >,\n",
       " <st 399 (tokenNum: 18) >,\n",
       " <st 400 (tokenNum: 17) >,\n",
       " <st 401 (tokenNum: 24) >,\n",
       " <st 402 (tokenNum: 17) >,\n",
       " <st 403 (tokenNum: 24) >,\n",
       " <st 404 (tokenNum: 58) >,\n",
       " <st 405 (tokenNum: 59) >,\n",
       " <st 406 (tokenNum: 90) >,\n",
       " <st 407 (tokenNum: 78) >,\n",
       " <st 408 (tokenNum: 68) >,\n",
       " <st 409 (tokenNum: 44) >,\n",
       " <st 410 (tokenNum: 25) >,\n",
       " <st 411 (tokenNum: 51) >,\n",
       " <st 412 (tokenNum: 25) >,\n",
       " <st 413 (tokenNum: 61) >,\n",
       " <st 414 (tokenNum: 15) >,\n",
       " <st 415 (tokenNum: 24) >,\n",
       " <st 416 (tokenNum: 76) >,\n",
       " <st 417 (tokenNum: 77) >,\n",
       " <st 418 (tokenNum: 45) >,\n",
       " <st 419 (tokenNum: 112) >,\n",
       " <st 420 (tokenNum: 85) >,\n",
       " <st 421 (tokenNum: 42) >,\n",
       " <st 422 (tokenNum: 10) >,\n",
       " <st 423 (tokenNum: 20) >,\n",
       " <st 424 (tokenNum: 16) >,\n",
       " <st 425 (tokenNum: 69) >,\n",
       " <st 426 (tokenNum: 39) >,\n",
       " <st 427 (tokenNum: 19) >,\n",
       " <st 428 (tokenNum: 26) >,\n",
       " <st 429 (tokenNum: 59) >,\n",
       " <st 430 (tokenNum: 54) >,\n",
       " <st 431 (tokenNum: 29) >,\n",
       " <st 432 (tokenNum: 57) >,\n",
       " <st 433 (tokenNum: 64) >,\n",
       " <st 434 (tokenNum: 63) >,\n",
       " <st 435 (tokenNum: 39) >,\n",
       " <st 436 (tokenNum: 36) >,\n",
       " <st 437 (tokenNum: 24) >,\n",
       " <st 438 (tokenNum: 43) >,\n",
       " <st 439 (tokenNum: 43) >,\n",
       " <st 440 (tokenNum: 74) >,\n",
       " <st 441 (tokenNum: 48) >,\n",
       " <st 442 (tokenNum: 42) >,\n",
       " <st 443 (tokenNum: 41) >,\n",
       " <st 444 (tokenNum: 62) >,\n",
       " <st 445 (tokenNum: 26) >,\n",
       " <st 446 (tokenNum: 16) >,\n",
       " <st 447 (tokenNum: 35) >,\n",
       " <st 448 (tokenNum: 32) >,\n",
       " <st 449 (tokenNum: 21) >,\n",
       " <st 450 (tokenNum: 53) >,\n",
       " <st 451 (tokenNum: 33) >,\n",
       " <st 452 (tokenNum: 16) >,\n",
       " <st 453 (tokenNum: 62) >,\n",
       " <st 454 (tokenNum: 43) >,\n",
       " <st 455 (tokenNum: 19) >,\n",
       " <st 456 (tokenNum: 67) >,\n",
       " <st 457 (tokenNum: 50) >,\n",
       " <st 458 (tokenNum: 29) >,\n",
       " <st 459 (tokenNum: 82) >,\n",
       " <st 460 (tokenNum: 11) >,\n",
       " <st 461 (tokenNum: 101) >,\n",
       " <st 462 (tokenNum: 85) >,\n",
       " <st 463 (tokenNum: 47) >,\n",
       " <st 464 (tokenNum: 58) >,\n",
       " <st 465 (tokenNum: 36) >,\n",
       " <st 466 (tokenNum: 13) >,\n",
       " <st 467 (tokenNum: 18) >,\n",
       " <st 468 (tokenNum: 18) >,\n",
       " <st 469 (tokenNum: 53) >,\n",
       " <st 470 (tokenNum: 62) >,\n",
       " <st 471 (tokenNum: 30) >,\n",
       " <st 472 (tokenNum: 16) >,\n",
       " <st 473 (tokenNum: 50) >,\n",
       " <st 474 (tokenNum: 34) >,\n",
       " <st 475 (tokenNum: 27) >,\n",
       " <st 476 (tokenNum: 31) >,\n",
       " <st 477 (tokenNum: 39) >,\n",
       " <st 478 (tokenNum: 24) >,\n",
       " <st 479 (tokenNum: 55) >,\n",
       " <st 480 (tokenNum: 43) >,\n",
       " <st 481 (tokenNum: 214) >,\n",
       " <st 482 (tokenNum: 82) >,\n",
       " <st 483 (tokenNum: 38) >,\n",
       " <st 484 (tokenNum: 145) >,\n",
       " <st 485 (tokenNum: 35) >,\n",
       " <st 486 (tokenNum: 99) >,\n",
       " <st 487 (tokenNum: 55) >,\n",
       " <st 488 (tokenNum: 45) >,\n",
       " <st 489 (tokenNum: 39) >,\n",
       " <st 490 (tokenNum: 37) >,\n",
       " <st 491 (tokenNum: 90) >,\n",
       " <st 492 (tokenNum: 55) >,\n",
       " <st 493 (tokenNum: 25) >,\n",
       " <st 494 (tokenNum: 127) >,\n",
       " <st 495 (tokenNum: 18) >,\n",
       " <st 496 (tokenNum: 63) >,\n",
       " <st 497 (tokenNum: 89) >,\n",
       " <st 498 (tokenNum: 86) >,\n",
       " <st 499 (tokenNum: 63) >,\n",
       " <st 500 (tokenNum: 89) >,\n",
       " <st 501 (tokenNum: 86) >,\n",
       " <st 502 (tokenNum: 63) >,\n",
       " <st 503 (tokenNum: 89) >,\n",
       " <st 504 (tokenNum: 86) >,\n",
       " <st 505 (tokenNum: 63) >,\n",
       " <st 506 (tokenNum: 89) >,\n",
       " <st 507 (tokenNum: 86) >,\n",
       " <st 508 (tokenNum: 63) >,\n",
       " <st 509 (tokenNum: 89) >,\n",
       " <st 510 (tokenNum: 86) >,\n",
       " <st 511 (tokenNum: 63) >,\n",
       " <st 512 (tokenNum: 89) >,\n",
       " <st 513 (tokenNum: 86) >,\n",
       " <st 514 (tokenNum: 63) >,\n",
       " <st 515 (tokenNum: 89) >,\n",
       " <st 516 (tokenNum: 86) >,\n",
       " <st 517 (tokenNum: 113) >,\n",
       " <st 518 (tokenNum: 142) >,\n",
       " <st 519 (tokenNum: 57) >,\n",
       " <st 520 (tokenNum: 33) >,\n",
       " <st 521 (tokenNum: 35) >,\n",
       " <st 522 (tokenNum: 69) >,\n",
       " <st 523 (tokenNum: 40) >,\n",
       " <st 524 (tokenNum: 51) >,\n",
       " <st 525 (tokenNum: 56) >,\n",
       " <st 526 (tokenNum: 37) >,\n",
       " <st 527 (tokenNum: 67) >,\n",
       " <st 528 (tokenNum: 81) >,\n",
       " <st 529 (tokenNum: 38) >,\n",
       " <st 530 (tokenNum: 57) >,\n",
       " <st 531 (tokenNum: 33) >,\n",
       " <st 532 (tokenNum: 18) >,\n",
       " <st 533 (tokenNum: 32) >,\n",
       " <st 534 (tokenNum: 22) >,\n",
       " <st 535 (tokenNum: 48) >,\n",
       " <st 536 (tokenNum: 48) >,\n",
       " <st 537 (tokenNum: 56) >,\n",
       " <st 538 (tokenNum: 27) >,\n",
       " <st 539 (tokenNum: 87) >,\n",
       " <st 540 (tokenNum: 35) >,\n",
       " <st 541 (tokenNum: 23) >,\n",
       " <st 542 (tokenNum: 26) >,\n",
       " <st 543 (tokenNum: 73) >,\n",
       " <st 544 (tokenNum: 40) >,\n",
       " <st 545 (tokenNum: 80) >,\n",
       " <st 546 (tokenNum: 38) >,\n",
       " <st 547 (tokenNum: 85) >,\n",
       " <st 548 (tokenNum: 35) >,\n",
       " <st 549 (tokenNum: 52) >,\n",
       " <st 550 (tokenNum: 74) >,\n",
       " <st 551 (tokenNum: 37) >,\n",
       " <st 552 (tokenNum: 39) >,\n",
       " <st 553 (tokenNum: 31) >,\n",
       " <st 554 (tokenNum: 25) >,\n",
       " <st 555 (tokenNum: 45) >,\n",
       " <st 556 (tokenNum: 34) >,\n",
       " <st 557 (tokenNum: 40) >,\n",
       " <st 558 (tokenNum: 58) >,\n",
       " <st 559 (tokenNum: 40) >,\n",
       " <st 560 (tokenNum: 38) >,\n",
       " <st 561 (tokenNum: 58) >,\n",
       " <st 562 (tokenNum: 83) >,\n",
       " <st 563 (tokenNum: 79) >,\n",
       " <st 564 (tokenNum: 64) >,\n",
       " <st 565 (tokenNum: 118) >,\n",
       " <st 566 (tokenNum: 17) >,\n",
       " <st 567 (tokenNum: 38) >,\n",
       " <st 568 (tokenNum: 76) >,\n",
       " <st 569 (tokenNum: 80) >,\n",
       " <st 570 (tokenNum: 48) >,\n",
       " <st 571 (tokenNum: 107) >,\n",
       " <st 572 (tokenNum: 37) >,\n",
       " <st 573 (tokenNum: 59) >,\n",
       " <st 574 (tokenNum: 46) >,\n",
       " <st 575 (tokenNum: 53) >,\n",
       " <st 576 (tokenNum: 99) >,\n",
       " <st 577 (tokenNum: 59) >,\n",
       " <st 578 (tokenNum: 28) >,\n",
       " <st 579 (tokenNum: 40) >,\n",
       " <st 580 (tokenNum: 39) >,\n",
       " <st 581 (tokenNum: 41) >,\n",
       " <st 582 (tokenNum: 23) >,\n",
       " <st 583 (tokenNum: 77) >,\n",
       " <st 584 (tokenNum: 50) >,\n",
       " <st 585 (tokenNum: 72) >,\n",
       " <st 586 (tokenNum: 33) >,\n",
       " <st 587 (tokenNum: 39) >,\n",
       " <st 588 (tokenNum: 34) >,\n",
       " <st 589 (tokenNum: 105) >,\n",
       " <st 590 (tokenNum: 39) >,\n",
       " <st 591 (tokenNum: 12) >,\n",
       " <st 592 (tokenNum: 164) >,\n",
       " <st 593 (tokenNum: 41) >,\n",
       " <st 594 (tokenNum: 35) >,\n",
       " <st 595 (tokenNum: 76) >,\n",
       " <st 596 (tokenNum: 40) >,\n",
       " <st 597 (tokenNum: 18) >,\n",
       " <st 598 (tokenNum: 25) >,\n",
       " <st 599 (tokenNum: 39) >,\n",
       " <st 600 (tokenNum: 21) >,\n",
       " <st 601 (tokenNum: 22) >,\n",
       " <st 602 (tokenNum: 41) >,\n",
       " <st 603 (tokenNum: 35) >,\n",
       " <st 604 (tokenNum: 76) >,\n",
       " <st 605 (tokenNum: 40) >,\n",
       " <st 606 (tokenNum: 18) >,\n",
       " <st 607 (tokenNum: 25) >,\n",
       " <st 608 (tokenNum: 39) >,\n",
       " <st 609 (tokenNum: 21) >,\n",
       " <st 610 (tokenNum: 22) >,\n",
       " <st 611 (tokenNum: 40) >,\n",
       " <st 612 (tokenNum: 59) >,\n",
       " <st 613 (tokenNum: 14) >,\n",
       " <st 614 (tokenNum: 55) >,\n",
       " <st 615 (tokenNum: 62) >,\n",
       " <st 616 (tokenNum: 59) >,\n",
       " <st 617 (tokenNum: 37) >,\n",
       " <st 618 (tokenNum: 36) >,\n",
       " <st 619 (tokenNum: 29) >,\n",
       " <st 620 (tokenNum: 20) >,\n",
       " <st 621 (tokenNum: 47) >,\n",
       " <st 622 (tokenNum: 34) >,\n",
       " <st 623 (tokenNum: 68) >,\n",
       " <st 624 (tokenNum: 9) >,\n",
       " <st 625 (tokenNum: 80) >,\n",
       " <st 626 (tokenNum: 59) >,\n",
       " <st 627 (tokenNum: 21) >,\n",
       " <st 628 (tokenNum: 37) >,\n",
       " <st 629 (tokenNum: 33) >,\n",
       " <st 630 (tokenNum: 118) >,\n",
       " <st 631 (tokenNum: 22) >,\n",
       " <st 632 (tokenNum: 37) >,\n",
       " <st 633 (tokenNum: 33) >,\n",
       " <st 634 (tokenNum: 64) >,\n",
       " <st 635 (tokenNum: 34) >,\n",
       " <st 636 (tokenNum: 27) >,\n",
       " <st 637 (tokenNum: 66) >,\n",
       " <st 638 (tokenNum: 46) >,\n",
       " <st 639 (tokenNum: 107) >,\n",
       " <st 640 (tokenNum: 71) >,\n",
       " <st 641 (tokenNum: 108) >,\n",
       " <st 642 (tokenNum: 51) >,\n",
       " <st 643 (tokenNum: 157) >,\n",
       " <st 644 (tokenNum: 30) >,\n",
       " <st 645 (tokenNum: 55) >,\n",
       " <st 646 (tokenNum: 54) >,\n",
       " <st 647 (tokenNum: 65) >,\n",
       " <st 648 (tokenNum: 66) >,\n",
       " <st 649 (tokenNum: 155) >,\n",
       " <st 650 (tokenNum: 137) >,\n",
       " <st 651 (tokenNum: 156) >,\n",
       " <st 652 (tokenNum: 54) >,\n",
       " <st 653 (tokenNum: 148) >,\n",
       " <st 654 (tokenNum: 93) >,\n",
       " <st 655 (tokenNum: 37) >,\n",
       " <st 656 (tokenNum: 23) >,\n",
       " <st 657 (tokenNum: 43) >,\n",
       " <st 658 (tokenNum: 79) >,\n",
       " <st 659 (tokenNum: 27) >,\n",
       " <st 660 (tokenNum: 50) >,\n",
       " <st 661 (tokenNum: 49) >,\n",
       " <st 662 (tokenNum: 49) >,\n",
       " <st 663 (tokenNum: 41) >,\n",
       " <st 664 (tokenNum: 33) >,\n",
       " <st 665 (tokenNum: 28) >,\n",
       " <st 666 (tokenNum: 43) >,\n",
       " <st 667 (tokenNum: 8) >,\n",
       " <st 668 (tokenNum: 42) >,\n",
       " <st 669 (tokenNum: 10) >,\n",
       " <st 670 (tokenNum: 18) >,\n",
       " <st 671 (tokenNum: 8) >,\n",
       " <st 672 (tokenNum: 14) >,\n",
       " <st 673 (tokenNum: 7) >,\n",
       " <st 674 (tokenNum: 18) >,\n",
       " <st 675 (tokenNum: 27) >,\n",
       " <st 676 (tokenNum: 54) >,\n",
       " <st 677 (tokenNum: 24) >,\n",
       " <st 678 (tokenNum: 18) >,\n",
       " <st 679 (tokenNum: 43) >,\n",
       " <st 680 (tokenNum: 49) >,\n",
       " <st 681 (tokenNum: 11) >,\n",
       " <st 682 (tokenNum: 25) >,\n",
       " <st 683 (tokenNum: 26) >,\n",
       " <st 684 (tokenNum: 46) >,\n",
       " <st 685 (tokenNum: 25) >,\n",
       " <st 686 (tokenNum: 83) >,\n",
       " <st 687 (tokenNum: 66) >,\n",
       " <st 688 (tokenNum: 43) >,\n",
       " <st 689 (tokenNum: 33) >,\n",
       " <st 690 (tokenNum: 31) >,\n",
       " <st 691 (tokenNum: 31) >,\n",
       " <st 692 (tokenNum: 24) >,\n",
       " <st 693 (tokenNum: 42) >,\n",
       " <st 694 (tokenNum: 17) >,\n",
       " <st 695 (tokenNum: 49) >,\n",
       " <st 696 (tokenNum: 59) >,\n",
       " <st 697 (tokenNum: 14) >,\n",
       " <st 698 (tokenNum: 20) >,\n",
       " <st 699 (tokenNum: 31) >,\n",
       " <st 700 (tokenNum: 34) >,\n",
       " <st 701 (tokenNum: 18) >,\n",
       " <st 702 (tokenNum: 43) >,\n",
       " <st 703 (tokenNum: 34) >,\n",
       " <st 704 (tokenNum: 48) >,\n",
       " <st 705 (tokenNum: 41) >,\n",
       " <st 706 (tokenNum: 48) >,\n",
       " <st 707 (tokenNum: 17) >,\n",
       " <st 708 (tokenNum: 24) >,\n",
       " <st 709 (tokenNum: 64) >,\n",
       " <st 710 (tokenNum: 79) >,\n",
       " <st 711 (tokenNum: 46) >,\n",
       " <st 712 (tokenNum: 21) >,\n",
       " <st 713 (tokenNum: 28) >,\n",
       " <st 714 (tokenNum: 49) >,\n",
       " <st 715 (tokenNum: 54) >,\n",
       " <st 716 (tokenNum: 68) >,\n",
       " <st 717 (tokenNum: 77) >,\n",
       " <st 718 (tokenNum: 40) >,\n",
       " <st 719 (tokenNum: 90) >,\n",
       " <st 720 (tokenNum: 41) >,\n",
       " <st 721 (tokenNum: 42) >,\n",
       " <st 722 (tokenNum: 31) >,\n",
       " <st 723 (tokenNum: 48) >,\n",
       " <st 724 (tokenNum: 25) >,\n",
       " <st 725 (tokenNum: 44) >,\n",
       " <st 726 (tokenNum: 21) >,\n",
       " <st 727 (tokenNum: 39) >,\n",
       " <st 728 (tokenNum: 46) >,\n",
       " <st 729 (tokenNum: 36) >,\n",
       " <st 730 (tokenNum: 45) >,\n",
       " <st 731 (tokenNum: 34) >,\n",
       " <st 732 (tokenNum: 35) >,\n",
       " <st 733 (tokenNum: 31) >,\n",
       " <st 734 (tokenNum: 66) >,\n",
       " <st 735 (tokenNum: 39) >,\n",
       " <st 736 (tokenNum: 41) >,\n",
       " <st 737 (tokenNum: 16) >,\n",
       " <st 738 (tokenNum: 22) >,\n",
       " <st 739 (tokenNum: 35) >,\n",
       " <st 740 (tokenNum: 58) >,\n",
       " <st 741 (tokenNum: 31) >,\n",
       " <st 742 (tokenNum: 63) >,\n",
       " <st 743 (tokenNum: 51) >,\n",
       " <st 744 (tokenNum: 54) >,\n",
       " <st 745 (tokenNum: 25) >,\n",
       " <st 746 (tokenNum: 78) >,\n",
       " <st 747 (tokenNum: 69) >,\n",
       " <st 748 (tokenNum: 62) >,\n",
       " <st 749 (tokenNum: 51) >,\n",
       " <st 750 (tokenNum: 53) >,\n",
       " <st 751 (tokenNum: 81) >,\n",
       " <st 752 (tokenNum: 70) >,\n",
       " <st 753 (tokenNum: 110) >,\n",
       " <st 754 (tokenNum: 62) >,\n",
       " <st 755 (tokenNum: 104) >,\n",
       " <st 756 (tokenNum: 39) >,\n",
       " <st 757 (tokenNum: 12) >,\n",
       " <st 758 (tokenNum: 38) >,\n",
       " <st 759 (tokenNum: 37) >,\n",
       " <st 760 (tokenNum: 22) >,\n",
       " <st 761 (tokenNum: 51) >,\n",
       " <st 762 (tokenNum: 47) >,\n",
       " <st 763 (tokenNum: 57) >,\n",
       " <st 764 (tokenNum: 45) >,\n",
       " <st 765 (tokenNum: 59) >,\n",
       " <st 766 (tokenNum: 40) >,\n",
       " <st 767 (tokenNum: 15) >,\n",
       " <st 768 (tokenNum: 82) >,\n",
       " <st 769 (tokenNum: 29) >,\n",
       " <st 770 (tokenNum: 11) >,\n",
       " <st 771 (tokenNum: 53) >,\n",
       " <st 772 (tokenNum: 28) >,\n",
       " <st 773 (tokenNum: 16) >,\n",
       " <st 774 (tokenNum: 128) >,\n",
       " <st 775 (tokenNum: 39) >,\n",
       " <st 776 (tokenNum: 128) >,\n",
       " <st 777 (tokenNum: 74) >,\n",
       " <st 778 (tokenNum: 27) >,\n",
       " <st 779 (tokenNum: 133) >,\n",
       " <st 780 (tokenNum: 82) >,\n",
       " <st 781 (tokenNum: 36) >,\n",
       " <st 782 (tokenNum: 40) >,\n",
       " <st 783 (tokenNum: 71) >,\n",
       " <st 784 (tokenNum: 48) >,\n",
       " <st 785 (tokenNum: 59) >,\n",
       " <st 786 (tokenNum: 38) >,\n",
       " <st 787 (tokenNum: 47) >,\n",
       " <st 788 (tokenNum: 79) >,\n",
       " <st 789 (tokenNum: 38) >,\n",
       " <st 790 (tokenNum: 82) >,\n",
       " <st 791 (tokenNum: 35) >,\n",
       " <st 792 (tokenNum: 18) >,\n",
       " <st 793 (tokenNum: 53) >,\n",
       " <st 794 (tokenNum: 19) >,\n",
       " <st 795 (tokenNum: 37) >,\n",
       " <st 796 (tokenNum: 30) >,\n",
       " <st 797 (tokenNum: 46) >,\n",
       " <st 798 (tokenNum: 30) >,\n",
       " <st 799 (tokenNum: 38) >,\n",
       " <st 800 (tokenNum: 48) >,\n",
       " <st 801 (tokenNum: 38) >,\n",
       " <st 802 (tokenNum: 101) >,\n",
       " <st 803 (tokenNum: 42) >,\n",
       " <st 804 (tokenNum: 40) >,\n",
       " <st 805 (tokenNum: 44) >,\n",
       " <st 806 (tokenNum: 40) >,\n",
       " <st 807 (tokenNum: 47) >,\n",
       " <st 808 (tokenNum: 43) >,\n",
       " <st 809 (tokenNum: 59) >,\n",
       " <st 810 (tokenNum: 56) >,\n",
       " <st 811 (tokenNum: 124) >,\n",
       " <st 812 (tokenNum: 41) >,\n",
       " <st 813 (tokenNum: 39) >,\n",
       " <st 814 (tokenNum: 105) >,\n",
       " <st 815 (tokenNum: 87) >,\n",
       " <st 816 (tokenNum: 66) >,\n",
       " <st 817 (tokenNum: 45) >,\n",
       " <st 818 (tokenNum: 23) >,\n",
       " <st 819 (tokenNum: 72) >,\n",
       " <st 820 (tokenNum: 38) >,\n",
       " <st 821 (tokenNum: 41) >,\n",
       " <st 822 (tokenNum: 36) >,\n",
       " <st 823 (tokenNum: 39) >,\n",
       " <st 824 (tokenNum: 19) >,\n",
       " <st 825 (tokenNum: 34) >,\n",
       " <st 826 (tokenNum: 64) >,\n",
       " <st 827 (tokenNum: 51) >,\n",
       " <st 828 (tokenNum: 58) >,\n",
       " <st 829 (tokenNum: 40) >,\n",
       " <st 830 (tokenNum: 95) >,\n",
       " <st 831 (tokenNum: 77) >,\n",
       " <st 832 (tokenNum: 101) >,\n",
       " <st 833 (tokenNum: 53) >,\n",
       " <st 834 (tokenNum: 35) >,\n",
       " <st 835 (tokenNum: 42) >,\n",
       " <st 836 (tokenNum: 25) >,\n",
       " <st 837 (tokenNum: 63) >,\n",
       " <st 838 (tokenNum: 29) >,\n",
       " <st 839 (tokenNum: 125) >,\n",
       " <st 840 (tokenNum: 49) >,\n",
       " <st 841 (tokenNum: 56) >,\n",
       " <st 842 (tokenNum: 20) >,\n",
       " <st 843 (tokenNum: 31) >,\n",
       " <st 844 (tokenNum: 32) >,\n",
       " <st 845 (tokenNum: 39) >,\n",
       " <st 846 (tokenNum: 80) >,\n",
       " <st 847 (tokenNum: 45) >,\n",
       " <st 848 (tokenNum: 74) >,\n",
       " <st 849 (tokenNum: 57) >,\n",
       " <st 850 (tokenNum: 42) >,\n",
       " <st 851 (tokenNum: 53) >,\n",
       " <st 852 (tokenNum: 93) >,\n",
       " <st 853 (tokenNum: 196) >,\n",
       " <st 854 (tokenNum: 71) >,\n",
       " <st 855 (tokenNum: 53) >,\n",
       " <st 856 (tokenNum: 104) >,\n",
       " <st 857 (tokenNum: 126) >,\n",
       " <st 858 (tokenNum: 50) >,\n",
       " <st 859 (tokenNum: 34) >,\n",
       " <st 860 (tokenNum: 29) >,\n",
       " <st 861 (tokenNum: 65) >,\n",
       " <st 862 (tokenNum: 45) >,\n",
       " <st 863 (tokenNum: 170) >,\n",
       " <st 864 (tokenNum: 65) >,\n",
       " <st 865 (tokenNum: 30) >,\n",
       " <st 866 (tokenNum: 30) >,\n",
       " <st 867 (tokenNum: 27) >,\n",
       " <st 868 (tokenNum: 30) >,\n",
       " <st 869 (tokenNum: 22) >,\n",
       " <st 870 (tokenNum: 14) >,\n",
       " <st 871 (tokenNum: 42) >,\n",
       " <st 872 (tokenNum: 57) >,\n",
       " <st 873 (tokenNum: 29) >,\n",
       " <st 874 (tokenNum: 45) >,\n",
       " <st 875 (tokenNum: 41) >,\n",
       " <st 876 (tokenNum: 39) >,\n",
       " <st 877 (tokenNum: 23) >,\n",
       " <st 878 (tokenNum: 13) >,\n",
       " <st 879 (tokenNum: 45) >,\n",
       " <st 880 (tokenNum: 27) >,\n",
       " <st 881 (tokenNum: 56) >,\n",
       " <st 882 (tokenNum: 54) >,\n",
       " <st 883 (tokenNum: 51) >,\n",
       " <st 884 (tokenNum: 78) >,\n",
       " <st 885 (tokenNum: 48) >,\n",
       " <st 886 (tokenNum: 117) >,\n",
       " <st 887 (tokenNum: 94) >,\n",
       " <st 888 (tokenNum: 52) >,\n",
       " <st 889 (tokenNum: 21) >,\n",
       " <st 890 (tokenNum: 55) >,\n",
       " <st 891 (tokenNum: 25) >,\n",
       " <st 892 (tokenNum: 50) >,\n",
       " <st 893 (tokenNum: 34) >,\n",
       " <st 894 (tokenNum: 47) >,\n",
       " <st 895 (tokenNum: 31) >,\n",
       " <st 896 (tokenNum: 61) >,\n",
       " <st 897 (tokenNum: 23) >,\n",
       " <st 898 (tokenNum: 154) >,\n",
       " <st 899 (tokenNum: 46) >,\n",
       " <st 900 (tokenNum: 76) >,\n",
       " <st 901 (tokenNum: 33) >,\n",
       " <st 902 (tokenNum: 19) >,\n",
       " <st 903 (tokenNum: 23) >,\n",
       " <st 904 (tokenNum: 18) >,\n",
       " <st 905 (tokenNum: 9) >,\n",
       " <st 906 (tokenNum: 15) >,\n",
       " <st 907 (tokenNum: 24) >,\n",
       " <st 908 (tokenNum: 37) >,\n",
       " <st 909 (tokenNum: 109) >,\n",
       " <st 910 (tokenNum: 12) >,\n",
       " <st 911 (tokenNum: 62) >,\n",
       " <st 912 (tokenNum: 32) >,\n",
       " <st 913 (tokenNum: 105) >,\n",
       " <st 914 (tokenNum: 31) >,\n",
       " <st 915 (tokenNum: 29) >,\n",
       " <st 916 (tokenNum: 15) >,\n",
       " <st 917 (tokenNum: 27) >,\n",
       " <st 918 (tokenNum: 33) >,\n",
       " <st 919 (tokenNum: 62) >,\n",
       " <st 920 (tokenNum: 16) >,\n",
       " <st 921 (tokenNum: 24) >,\n",
       " <st 922 (tokenNum: 41) >,\n",
       " <st 923 (tokenNum: 11) >,\n",
       " <st 924 (tokenNum: 64) >,\n",
       " <st 925 (tokenNum: 63) >,\n",
       " <st 926 (tokenNum: 31) >,\n",
       " <st 927 (tokenNum: 34) >,\n",
       " <st 928 (tokenNum: 41) >,\n",
       " <st 929 (tokenNum: 56) >,\n",
       " <st 930 (tokenNum: 76) >,\n",
       " <st 931 (tokenNum: 63) >,\n",
       " <st 932 (tokenNum: 39) >,\n",
       " <st 933 (tokenNum: 117) >,\n",
       " <st 934 (tokenNum: 55) >,\n",
       " <st 935 (tokenNum: 194) >,\n",
       " <st 936 (tokenNum: 40) >,\n",
       " <st 937 (tokenNum: 52) >,\n",
       " <st 938 (tokenNum: 50) >,\n",
       " <st 939 (tokenNum: 32) >,\n",
       " <st 940 (tokenNum: 34) >,\n",
       " <st 941 (tokenNum: 19) >,\n",
       " <st 942 (tokenNum: 55) >,\n",
       " <st 943 (tokenNum: 45) >,\n",
       " <st 944 (tokenNum: 18) >,\n",
       " <st 945 (tokenNum: 23) >,\n",
       " <st 946 (tokenNum: 13) >,\n",
       " <st 947 (tokenNum: 38) >,\n",
       " <st 948 (tokenNum: 83) >,\n",
       " <st 949 (tokenNum: 24) >,\n",
       " <st 950 (tokenNum: 33) >,\n",
       " <st 951 (tokenNum: 62) >,\n",
       " <st 952 (tokenNum: 74) >,\n",
       " <st 953 (tokenNum: 24) >,\n",
       " <st 954 (tokenNum: 29) >,\n",
       " <st 955 (tokenNum: 65) >,\n",
       " <st 956 (tokenNum: 132) >,\n",
       " <st 957 (tokenNum: 115) >,\n",
       " <st 958 (tokenNum: 121) >,\n",
       " <st 959 (tokenNum: 69) >,\n",
       " <st 960 (tokenNum: 65) >,\n",
       " <st 961 (tokenNum: 46) >,\n",
       " <st 962 (tokenNum: 28) >,\n",
       " <st 963 (tokenNum: 46) >,\n",
       " <st 964 (tokenNum: 29) >,\n",
       " <st 965 (tokenNum: 47) >,\n",
       " <st 966 (tokenNum: 45) >,\n",
       " <st 967 (tokenNum: 35) >,\n",
       " <st 968 (tokenNum: 100) >,\n",
       " <st 969 (tokenNum: 26) >,\n",
       " <st 970 (tokenNum: 56) >,\n",
       " <st 971 (tokenNum: 27) >,\n",
       " <st 972 (tokenNum: 140) >,\n",
       " <st 973 (tokenNum: 178) >,\n",
       " <st 974 (tokenNum: 21) >,\n",
       " <st 975 (tokenNum: 112) >,\n",
       " <st 976 (tokenNum: 25) >,\n",
       " <st 977 (tokenNum: 75) >,\n",
       " <st 978 (tokenNum: 29) >,\n",
       " <st 979 (tokenNum: 81) >,\n",
       " <st 980 (tokenNum: 44) >,\n",
       " <st 981 (tokenNum: 72) >,\n",
       " <st 982 (tokenNum: 60) >,\n",
       " <st 983 (tokenNum: 31) >,\n",
       " <st 984 (tokenNum: 74) >,\n",
       " <st 985 (tokenNum: 60) >,\n",
       " <st 986 (tokenNum: 40) >,\n",
       " <st 987 (tokenNum: 34) >,\n",
       " <st 988 (tokenNum: 28) >,\n",
       " <st 989 (tokenNum: 40) >,\n",
       " <st 990 (tokenNum: 34) >,\n",
       " <st 991 (tokenNum: 28) >,\n",
       " <st 992 (tokenNum: 30) >,\n",
       " <st 993 (tokenNum: 59) >,\n",
       " <st 994 (tokenNum: 24) >,\n",
       " <st 995 (tokenNum: 42) >,\n",
       " <st 996 (tokenNum: 55) >,\n",
       " <st 997 (tokenNum: 89) >,\n",
       " <st 998 (tokenNum: 56) >,\n",
       " <st 999 (tokenNum: 34) >,\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10214\n"
     ]
    }
   ],
   "source": [
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The num of train sentences: 6128\n",
      "The num of test  sentences: 2043\n",
      "The num of valid sentences: 2043\n"
     ]
    }
   ],
   "source": [
    "from crfpp.loaddata import get_train_test_valid\n",
    "\n",
    "total_sent_num = nlptext.SENT['length']\n",
    "train_sent_idx, test_sent_idx, valid_sent_idx = get_train_test_valid(total_sent_num, seed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = [sents[i] for i in train_sent_idx]\n",
    "test_sents  = [sents[i] for i in test_sent_idx]\n",
    "valid_sents = [sents[i] for i in valid_sent_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/boson/char/T'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.utils.channel import getChannelName\n",
    "\n",
    "def get_model_name(BasicObject, Channel_Settings):\n",
    "    return BasicObject.Data_Dir.replace('data/', 'model/') + '/' + \"_\".join([getChannelName(ch, style = 'abbr') for ch in Channel_Settings])\n",
    "# Channel_Settings\n",
    "model = BasicObject.Data_Dir.replace('data/', 'model/') + '/' + \"_\".join([getChannelName(ch, style = 'abbr') for ch in Channel_Settings])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from crfpp.crftools import crf_learn\n",
    "import pickle\n",
    "\n",
    "def crfpp_train(model, trainSents, Channel_Settings, feat_type = 'str', fieldembed = None):\n",
    "    \n",
    "    if 'str' in feat_type.lower():\n",
    "        get_sent_feats = get_sent_strfeats\n",
    "    elif 'vec' in feat_type.lower():\n",
    "        get_sent_feats = get_sent_vecfeats\n",
    "        \n",
    "    tmp_dir = model.replace('model', '_tmp')\n",
    "    \n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir)\n",
    "        \n",
    "    if not os.path.exists(model):\n",
    "        os.makedirs(model)\n",
    "        \n",
    "    model_path = model + '/model'\n",
    "    para_path  = model + '/para.p'\n",
    "    template_path = model + '/template'\n",
    "    feats_data_path = tmp_dir + '/feats.txt'\n",
    "    with open(para_path, 'wb') as f:\n",
    "        pickle.dump(Channel_Settings, f)\n",
    "    \n",
    "    DFtrain = []\n",
    "    if fieldembed:\n",
    "        Channel_Settings = fieldembed\n",
    "    for idx, sent in enumerate(trainSents):\n",
    "        if idx % 500 == 0:\n",
    "            print(datetime.now(), idx)\n",
    "        df = get_sent_feats(sent, Channel_Settings)\n",
    "        df.loc[len(df)] = np.NaN     ## Trick Here\n",
    "        DFtrain.append(df)           ## Trick Here\n",
    "    DFtrain = pd.concat(DFtrain).reset_index(drop=True)\n",
    "    DFtrain.to_csv(feats_data_path, sep = '\\t', encoding = 'utf=8', header = False, index = False )\n",
    "    \n",
    "    generate_template(input_feats_num = DFtrain.shape[1] - 1, path = template_path) \n",
    "    crf_learn(feats_data_path, model_path, template_path  = template_path)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': {'Max_Ngram': 1}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Channel_Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-31 16:17:17.578930 0\n",
      "2019-07-31 16:17:18.615698 500\n",
      "2019-07-31 16:17:19.651774 1000\n",
      "2019-07-31 16:17:20.699888 1500\n",
      "2019-07-31 16:17:21.688091 2000\n",
      "2019-07-31 16:17:22.693033 2500\n",
      "2019-07-31 16:17:23.909671 3000\n",
      "2019-07-31 16:17:24.913635 3500\n",
      "2019-07-31 16:17:25.890951 4000\n",
      "2019-07-31 16:17:27.001635 4500\n",
      "2019-07-31 16:17:27.971038 5000\n",
      "2019-07-31 16:17:29.061359 5500\n",
      "2019-07-31 16:17:30.063687 6000\n",
      "CRF++: Yet Another CRF Tool Kit\n",
      "Copyright (C) 2005-2013 Taku Kudo, All rights reserved.\n",
      "\n",
      "reading training data: \n",
      "Done!11.93 s\n",
      "\n",
      "Number of sentences: 1\n",
      "Number of features:  12524521\n",
      "Number of thread(s): 8\n",
      "Freq:                2\n",
      "eta:                 0.00010\n",
      "C:                   5.00000\n",
      "shrinking size:      20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crfpp_train(model, train_sents, Channel_Settings, feat_type = 'str', fieldembed = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<st 103 (tokenNum: 312) >\n",
      "一 到 节 假 日 , 不 论 是 旅 游 景 点 还 是 餐 厅 、 K T V 等 地 都 是 人 满 为 患 , 本 打 算 团 购 一 个 美 食 套 餐 , 在 假 期 约 上 三 五 好 友 共 享 一 番 , 然 而 多 次 预 约 都 已 满 , 多 位 网 友 均 向 本 平 台 反 映 团 购 的 套 餐 由 于 遇 上 长 假 消 费 高 峰 因 此 眼 睁 睁 看 着 过 期 , 消 费 不 成 , 退 款 也 无 门 。 ( 相 关 案 例 详 见 : 订 不 到 餐 是 否 可 退 窝 窝 网 、 澳 门 豆 捞 说 法 不 一 1 0 0 e c . c n / d e t a i l - - 6 0 0 3 0 3 5 . h t m l ) 经 常 网 购 的 朋 友 一 定 发 现 了 促 销 期 间 的 发 货 时 间 是 平 常 的 3 - 4 倍 , 由 于 促 销 期 间 , 商 家 订 单 数 量 大 , 往 往 就 发 货 不 及 时 , 因 此 , 网 友 下 单 后 本 来 两 三 天 能 收 到 的 商 品 可 能 十 天 也 未 必 能 收 到 。 ( 相 关 案 例 详 见 : 搜 狐 爱 家 团 付 款 订 单 不 发 货 显 示 状 态 未 支 付 1 0 0 e c . c n / d e t a i l - - 6 0 0 3 0 3 8 . h t m l )\n",
      "model/boson/char/T_E-bio\n"
     ]
    }
   ],
   "source": [
    "sent = testSents[21]\n",
    "print(sent)\n",
    "print(sent.sentence)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 5, 'time'),\n",
       " (18, 21, 'company_name'),\n",
       " (129, 132, 'product_name'),\n",
       " (133, 135, 'location'),\n",
       " (263, 268, 'product_name'),\n",
       " (309, 311, 'person_name')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crfpp.crftools import get_sent_strfeats, crf_test\n",
    "from crfpp.evals import read_target_seq, extractSET\n",
    "\n",
    "def tagger(model, sent, Channel_Settings = None):\n",
    "    '''\n",
    "        basically from crf_test\n",
    "        sent: a sentence, could be without annotation\n",
    "    '''\n",
    "    if not Channel_Settings:\n",
    "        with open(model + '/para.p', 'rb') as f:\n",
    "            Channel_Settings = pickle.load(f)\n",
    "    # 1. get sentence feats\n",
    "    # hopefully, the model_config is included in model_path\n",
    "    feats_data_path   = '_tmp/_tagger_feats.txt'\n",
    "    results_data_path = '_tmp/_tagger_results.txt'\n",
    "    model_path = model + '/model'\n",
    "\n",
    "    # get Channel_Settings\n",
    "    # get use sent strfeats or sent vecfeats settings\n",
    "    df = get_sent_strfeats(sent, Channel_Settings, train = False)\n",
    "    df.to_csv(feats_data_path, sep = '\\t', encoding = 'utf=8', header = False, index = False )\n",
    "    # 2. save the sentence feats to a file\n",
    "    \n",
    "    # 3. tag a sentence\n",
    "    crf_test(feats_data_path, model_path, results_data_path)\n",
    "\n",
    "    # 4. read and parse the result to pred_SSET\n",
    "    # get a tag_seq\n",
    "    # list of tuple (score, result)\n",
    "    tag_seq  = read_target_seq(results_data_path)\n",
    "    pred_SET = extractSET(tag_seq)\n",
    "    return pred_SET\n",
    "\n",
    "tagger(model, sent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company_name', 'location', 'org_name', 'person_name', 'product_name', 'time']\n",
      "model/boson/char/T_E-bio\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "###################################################  from IT to SET \n",
    "\n",
    "def read_target_seq(result_path):\n",
    "    result = pd.read_csv(result_path, sep = '\\t', header = None, skip_blank_lines=False)\n",
    "    # get the last column of the result\n",
    "    return result.iloc[:,-1].dropna().values\n",
    "\n",
    "\n",
    "def extractSET(tag_seq, exist_SE = False):\n",
    "    '''\n",
    "        SET: start, end, tag\n",
    "        tag_seq: the hyper field sequence for this sentence\n",
    "\n",
    "    '''\n",
    "    if exist_SE:\n",
    "        tag_seq = tag_seq[1:-1]\n",
    "\n",
    "    IT = list(zip(range(len(tag_seq)), tag_seq))\n",
    "    taggedIT = [it for it in IT if it[1]!= 'O']\n",
    "    \n",
    "    startIdx = [idx for idx in range(len(taggedIT)) if taggedIT[idx][1][-2:] == '-B' or taggedIT[idx][1][-2:] == '-S']\n",
    "    startIdx.append(len(taggedIT))\n",
    "\n",
    "    entitiesList = []\n",
    "    for i in range(len(startIdx)-1):\n",
    "        entityAtom = taggedIT[startIdx[i]: startIdx[i+1]]\n",
    "        # string = ''.join([cit[0] for cit in entityAtom])\n",
    "        start, end = entityAtom[0][0], entityAtom[-1][0] + 1\n",
    "        tag = entityAtom[0][1].split('-')[0]\n",
    "        entitiesList.append((start, end, tag))\n",
    "    return entitiesList\n",
    "\n",
    "\n",
    "def get_sent_annoSET(sent, channel = 'annoE', tagScheme = 'BIO'):\n",
    "    anno_seq = [i[0] for i in sent.get_grain_str(channel, tagScheme=tagScheme)]\n",
    "    anno_SET = extractSET(anno_seq)\n",
    "    return anno_SET \n",
    "\n",
    "\n",
    "\n",
    "###################################################  compare pred_SET and anno_SET\n",
    "\n",
    "def match_anno_pred_result(anno_entities, pred_entities, labels = []):\n",
    "    '''\n",
    "        anno_entities, pred_entities of a batch of sentences\n",
    "    '''\n",
    "    if type(anno_entities[0]) != list:\n",
    "        anno_entities = [anno_entities]\n",
    "        pred_entities = [pred_entities]\n",
    "        \n",
    "    name_list = ['E@Anno', 'E@Pred',  'E@Match']\n",
    "    for eL in labels:\n",
    "        name_list.extend([eL + suff for suff in ['@Anno', '@Pred', '@Match']])\n",
    "    \n",
    "    statistic_result = []\n",
    "    \n",
    "    for idx in range(len(pred_entities)):\n",
    "        pred = set(pred_entities[idx])\n",
    "        anno = set(anno_entities[idx])\n",
    "        d = {'E@Pred'  : len(pred),\n",
    "             'E@Anno'  : len(anno),\n",
    "             'E@Match' : len(pred.intersection(anno))}\n",
    "        \n",
    "        for eL in labels:\n",
    "            elL = [e for e in pred if eL == e[-1]]\n",
    "            elA = [e for e in anno if eL == e[-1]]\n",
    "            elM = set(elA).intersection(set(elL)) ## Union vs Join\n",
    "            d[eL+'@Pred'] = len(elL)\n",
    "            d[eL+'@Anno'] = len(elA)\n",
    "            d[eL+'@Match'] = len(elM)\n",
    "        \n",
    "        statistic_result.append(d)\n",
    "    Result = pd.DataFrame(statistic_result)[name_list]\n",
    "    return Result\n",
    "\n",
    "\n",
    "def calculate_F1_Score(Result, labels):\n",
    "    if len(Result.shape) == 1:\n",
    "        Result = Result.to_dict()\n",
    "    else:\n",
    "        Result = Result.sum().to_dict()\n",
    "    List = []\n",
    "    entitiesLabel = labels + ['E']\n",
    "    # entitiesLabel = ['Sy','Bo', 'Ch', 'Tr', 'Si'] + ['R'] + ['E']\n",
    "    for eL in entitiesLabel:\n",
    "        d = dict()\n",
    "        d['id'] = eL\n",
    "        for k in Result:\n",
    "            if eL == k.split('@')[0]:\n",
    "                d[k.replace(eL + '@','')] = Result[k]\n",
    "        List.append(d)\n",
    "    \n",
    "    # print(List)\n",
    "    R = pd.DataFrame(List)\n",
    "    R.set_index('id', inplace = True)\n",
    "    R.index.name = None\n",
    "    # print(R)\n",
    "    R['R'] = R['Match']/R['Anno']\n",
    "    R['P'] = R['Match']/R['Pred']\n",
    "    R['F1'] = 2*R['R']*R['P']/(R['R'] + R['P'])\n",
    "    return R[['Anno', 'Pred', 'Match', 'R', 'P', 'F1']]\n",
    "\n",
    "\n",
    "################################################### log the errors between pred_SET and anno_SET\n",
    "\n",
    "def matchPaired(L, A, sent):\n",
    "    start1, end1, e1 = L\n",
    "    start2, end2, e2 = A\n",
    "    d = {}\n",
    "    sentence = sent.sentence\n",
    "    if set(range(start1, end1+1)).intersection(range(start2, end2+1)):\n",
    "        idx = set(range(start1, end1+1)).union(range(start2, end2+1))\n",
    "        # print()\n",
    "        d['text_part'] = sent.sentence[min(idx): max(idx) + 1]\n",
    "        d['start'] = min(idx)\n",
    "        d['end' ]  = max(idx) \n",
    "        d['pred'] = sent.sentence[start1: end1]\n",
    "        d['pred_en'] = e1\n",
    "        d['anno'] = sent.sentence[start2: end2]\n",
    "        d['anno_en'] = e2\n",
    "        d['sent_idx']= sent.Idx # this is important\n",
    "        return d\n",
    "    \n",
    "def matchUnpaired(unpaired, sent, kind):\n",
    "    d = {}\n",
    "    sentence = sent.sentence\n",
    "    d['start'], d['end' ], e = unpaired\n",
    "    d['text_part'] = sentence[d['start']: d['end' ]]\n",
    "    d['sent_idx']= sent.Idx\n",
    "    if kind == 'P':\n",
    "        d['pred'], d['pred_en'] = d['text_part'], e\n",
    "        d['anno'], d['anno_en'] = None, None\n",
    "    else:\n",
    "        d['pred'], d['pred_en'] = None, None\n",
    "        d['anno'], d['anno_en'] = d['text_part'], e\n",
    "    return d\n",
    "\n",
    "\n",
    "def logErrors(sent, anno_enetities, pred_entities):\n",
    "    log = []\n",
    "    inter = list(set(pred_entities).intersection(set(anno_enetities)))\n",
    "    only_pred = [ i for i in pred_entities if i not in inter]   \n",
    "    only_anno = [ i for i in anno_enetities if i not in inter]\n",
    "    \n",
    "    pairedP = []\n",
    "    pairedA = []\n",
    "    for L in only_pred:\n",
    "        for A in only_anno:\n",
    "            d = matchPaired(L, A, sent)\n",
    "            if d:\n",
    "                log.append(d)\n",
    "                pairedP.append(L)\n",
    "                pairedA.append(A)\n",
    "                \n",
    "    for L in [i for i in only_pred if i not in pairedP]:\n",
    "        log.append(matchUnpaired(L, sent, 'P'))\n",
    "        \n",
    "    for A in [i for i in only_anno if i not in pairedA]:\n",
    "        log.append(matchUnpaired(A, sent, 'A'))\n",
    "           \n",
    "    if len(log) == 0:\n",
    "        return pd.DataFrame()\n",
    "    cols = ['sent_idx', 'text_part', 'start', 'end', 'anno', 'anno_en', 'pred', 'pred_en']\n",
    "    return pd.DataFrame(log)[cols].sort_values('start')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 17:15:21.014586 0\n",
      "2019-07-22 17:15:52.238445 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anno</th>\n",
       "      <th>Pred</th>\n",
       "      <th>Match</th>\n",
       "      <th>R</th>\n",
       "      <th>P</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company_name</th>\n",
       "      <td>400</td>\n",
       "      <td>307</td>\n",
       "      <td>204</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.664495</td>\n",
       "      <td>0.577086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>900</td>\n",
       "      <td>806</td>\n",
       "      <td>596</td>\n",
       "      <td>0.662222</td>\n",
       "      <td>0.739454</td>\n",
       "      <td>0.698710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org_name</th>\n",
       "      <td>561</td>\n",
       "      <td>381</td>\n",
       "      <td>295</td>\n",
       "      <td>0.525847</td>\n",
       "      <td>0.774278</td>\n",
       "      <td>0.626327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person_name</th>\n",
       "      <td>1063</td>\n",
       "      <td>805</td>\n",
       "      <td>731</td>\n",
       "      <td>0.687676</td>\n",
       "      <td>0.908075</td>\n",
       "      <td>0.782655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_name</th>\n",
       "      <td>768</td>\n",
       "      <td>639</td>\n",
       "      <td>456</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.713615</td>\n",
       "      <td>0.648188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>885</td>\n",
       "      <td>815</td>\n",
       "      <td>663</td>\n",
       "      <td>0.749153</td>\n",
       "      <td>0.813497</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>4577</td>\n",
       "      <td>3753</td>\n",
       "      <td>2945</td>\n",
       "      <td>0.643435</td>\n",
       "      <td>0.784706</td>\n",
       "      <td>0.707083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Anno  Pred  Match         R         P        F1\n",
       "company_name   400   307    204  0.510000  0.664495  0.577086\n",
       "location       900   806    596  0.662222  0.739454  0.698710\n",
       "org_name       561   381    295  0.525847  0.774278  0.626327\n",
       "person_name   1063   805    731  0.687676  0.908075  0.782655\n",
       "product_name   768   639    456  0.593750  0.713615  0.648188\n",
       "time           885   815    663  0.749153  0.813497  0.780000\n",
       "E             4577  3753   2945  0.643435  0.784706  0.707083"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from crfpp.evals import get_sent_annoSET, match_anno_pred_result, calculate_F1_Score, logError\n",
    "\n",
    "def crfpp_test(model, testSents, Channel_Settings,  labels):\n",
    "    '''\n",
    "        sents: a list of sents\n",
    "        This function could be updated in the future for a better performance.\n",
    "        But currently, just leave it now.\n",
    "    '''\n",
    "    pred_entities = []\n",
    "    anno_entities = []\n",
    "    log_list      = []\n",
    "    # here sents are a batch of sents, not necessary to be all the sents\n",
    "    # this could be chanage, but it will cause some time, so just ignore it.\n",
    "    for idx, sent in enumerate(testSents):\n",
    "        if idx % 200 == 0:\n",
    "            print(datetime.now(), idx)\n",
    "        # 200/13s\n",
    "        pred_SET = tagger(model, sent, Channel_Settings = Channel_Settings)\n",
    "        pred_entities.append(pred_SET)\n",
    "        \n",
    "        anno_SET = get_sent_annoSET(sent)\n",
    "        anno_entities.append(anno_SET)\n",
    "        \n",
    "        error = logErrors(sent, pred_SET, anno_SET)\n",
    "        log_list.append(error)\n",
    "    # return anno_entities, pred_entities\n",
    "    Result = match_anno_pred_result(anno_entities, pred_entities, labels = labels)\n",
    "    # return Result\n",
    "    R = calculate_F1_Score(Result, labels)\n",
    "\n",
    "    LogError = pd.concat(log_list).reset_index(drop = True)\n",
    "    return R, LogError\n",
    "\n",
    "\n",
    "R, LogError = crfpp_test(model, testSents, Channel_Settings,  labels)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Overall Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/ResumeNER/test.char.bmes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/ResumeNER/train.char.bmes\n",
      "corpus/ResumeNER/dev.char.bmes\n",
      "Total Num of All    Tokens 149812\n",
      "The Total Number of Tokens: 149812\n",
      "Counting the number unique Tokens...          \t 2019-06-20 20:19:23.731256\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-06-20 20:19:23.777903\n",
      "\t\tThe length of DTU is: 1891 \t 2019-06-20 20:19:23.778263\n",
      "Generating the ORIGTokenIndex...       \t 2019-06-20 20:19:23.778312\n",
      "\t\tThe idx of token is: 0 \t 2019-06-20 20:19:23.778461\n",
      "\t\tDone!\n",
      "Only Keep First 3500000 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 1891\n",
      "['</pad>', '</start>', '</end>', 'O', 'CONT-B', 'CONT-E', 'CONT-I', 'CONT-S', 'EDU-B', 'EDU-E', 'EDU-I', 'EDU-S', 'LOC-B', 'LOC-E', 'LOC-I', 'LOC-S', 'NAME-B', 'NAME-E', 'NAME-I', 'NAME-S', 'ORG-B', 'ORG-E', 'ORG-I', 'ORG-S', 'PRO-B', 'PRO-E', 'PRO-I', 'PRO-S', 'RACE-B', 'RACE-E', 'RACE-I', 'RACE-S', 'TITLE-B', 'TITLE-E', 'TITLE-I', 'TITLE-S']\n",
      "CORPUS\tit is Dumped into file: data/ResumeNER/char/Token1891/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data/ResumeNER/char/Token1891/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 3\n",
      "TEXT\tit is Dumped into file: data/ResumeNER/char/Token1891/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4617\n",
      "SENT\tit is Dumped into file: data/ResumeNER/char/Token1891/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4617\n",
      "TOKEN\tit is Dumped into file: data/ResumeNER/char/Token1891/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 149812\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data/ResumeNER/char/Token1891/GrainUnique/token.voc\n",
      "token\tthe length of it is   : 1891\n",
      "\t\tWrite to: data/ResumeNER/char/Token1891/GrainUnique/token.tsv\n",
      "annoE-es\tis Dumped into file: data/ResumeNER/char/Token1891/GrainUnique/annoE-es.voc\n",
      "annoE-es\tthe length of it is   : 36\n",
      "\t\tWrite to: data/ResumeNER/char/Token1891/GrainUnique/annoE-es.tsv\n",
      "pos-es\tis Dumped into file: data/ResumeNER/char/Token1891/GrainUnique/pos-es.voc\n",
      "pos-es\tthe length of it is   : 232\n",
      "\t\tWrite to: data/ResumeNER/char/Token1891/GrainUnique/pos-es.tsv\n",
      "****************************************\n",
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: basic\n",
      "Current Channel is        \t basic\n",
      "Current Channel Max_Ngram \t 1\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: basic\n",
      "For channel: | basic | build GrainUnique and LookUp\n",
      "\t\tWrite to: data/ResumeNER/char/Token1891/GrainUnique/basic.voc\n",
      "\t\tWrite to: data/ResumeNER/char/Token1891/GrainUnique/basic.tsv\n",
      "\t\tWrite to: data/ResumeNER/char/Token1891/GrainUnique/basic.lkp\n",
      "Deal with the Channel: annoE\n",
      "Current Channel is        \t annoE\n",
      "Current Channel Max_Ngram \t 1\n",
      "\t\tBuild GrainUnique for channel: annoE\n",
      "annoE 1 False BIO\n",
      "\t\tWrite to: data/ResumeNER/char/Token1891/GrainUnique/annoE.voc\n",
      "\t\tWrite to: data/ResumeNER/char/Token1891/GrainUnique/annoE.tsv\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "########### ResumeNER ###########\n",
    "CORPUSPath = 'corpus/ResumeNER/'\n",
    "corpusFileIden = '.bmes'\n",
    "textType   = 'block'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'iter'\n",
    "TOKENLevel = 'char'\n",
    "anno = 'embed'\n",
    "annoKW = {}\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW)\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True,'Max_Ngram': 1,}, # always the char-level token\n",
    "    'basic':   {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'medical': {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'radical': {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'subcomp': {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'stroke':  {'use': False,'Max_Ngram': 1, 'end_grain': False},\n",
    "    # CTX_DEP\n",
    "    'pos':     {'use': False, 'tagScheme': 'BIO',},\n",
    "    # ANNO\n",
    "    'annoE':   {'use': True, 'tagScheme': 'BIO',},\n",
    "}\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(CHANNEL_SETTINGS_TEMPLATE=CHANNEL_SETTINGS_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': {'Max_Ngram': 1}, 'basic': {'Max_Ngram': 1, 'end_grain': False}, 'annoE': {'tagScheme': 'BIO'}}\n",
      "annoE\n",
      "BIO\n",
      "['CONT', 'EDU', 'LOC', 'NAME', 'ORG', 'PRO', 'RACE', 'TITLE']\n",
      "['</pad>', '</start>', '</end>', 'CONT-B', 'CONT-I', 'EDU-B', 'EDU-I', 'LOC-B', 'LOC-I', 'NAME-B', 'NAME-I', 'O', 'ORG-B', 'ORG-I', 'PRO-B', 'PRO-I', 'RACE-B', 'RACE-I', 'TITLE-B', 'TITLE-I']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from crfpp.crftools import load_anno\n",
    "\n",
    "anno_field = 'annoE'\n",
    "Channel_Settings, tagScheme, labels, tags, tag_size = load_anno(BasicObject, anno_field)\n",
    "print(Channel_Settings)\n",
    "print(anno_field)\n",
    "print(tagScheme)\n",
    "print(labels)\n",
    "print(tags)\n",
    "print(tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/ResumeNER/char/Token1891/T_b\n"
     ]
    }
   ],
   "source": [
    "from nlptext.utils.channel import getChannelName\n",
    "\n",
    "def get_model_name(BasicObject, Channel_Settings):\n",
    "    return BasicObject.TokenNum_Dir.replace('data/', 'model/') + '/' + \"_\".join([getChannelName(ch, style = 'abbr') for ch in Channel_Settings if 'anno' not in ch])\n",
    "# Channel_Settings\n",
    "\n",
    "model = get_model_name(BasicObject, Channel_Settings)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/ResumeNER/char/Token1891/T_b\n",
      "4617\n",
      "{'token': {'Max_Ngram': 1}, 'basic': {'Max_Ngram': 1, 'end_grain': False}, 'annoE': {'tagScheme': 'BIO'}}\n"
     ]
    }
   ],
   "source": [
    "from nlptext.corpus import Corpus\n",
    "corpus = Corpus()\n",
    "sents = corpus.Sentences\n",
    "\n",
    "print(model)\n",
    "print(len(sents))\n",
    "print(Channel_Settings)\n",
    "\n",
    "cross_num = 4\n",
    "cross_validation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-20 20:19:24.283458 0\n",
      "2019-06-20 20:19:25.604132 500\n",
      "2019-06-20 20:19:26.992641 1000\n",
      "2019-06-20 20:19:28.296988 1500\n",
      "2019-06-20 20:19:29.613115 2000\n",
      "2019-06-20 20:19:30.881008 2500\n",
      "2019-06-20 20:19:32.153796 3000\n",
      "CRF++: Yet Another CRF Tool Kit\n",
      "Copyright (C) 2005-2013 Taku Kudo, All rights reserved.\n",
      "\n",
      "reading training data: 100.. 200.. 300.. 400.. 500.. 600.. 700.. 800.. 900.. 1000.. 1100.. 1200.. 1300.. 1400.. 1500.. 1600.. 1700.. 1800.. 1900.. 2000.. 2100.. 2200.. 2300.. 2400.. 2500.. 2600.. 2700.. 2800.. 2900.. 3000.. 3100.. 3200.. 3300.. 3400.. \n",
      "Done!1.03 s\n",
      "\n",
      "Number of sentences: 3463\n",
      "Number of features:  1254770\n",
      "Number of thread(s): 8\n",
      "Freq:                2\n",
      "eta:                 0.00010\n",
      "C:                   5.00000\n",
      "shrinking size:      20\n",
      "iter=0 terr=0.99796 serr=1.00000 act=1254770 obj=321255.22787 diff=1.00000\n",
      "iter=1 terr=0.54466 serr=1.00000 act=1254770 obj=205289.10905 diff=0.36098\n",
      "iter=2 terr=0.37801 serr=1.00000 act=1254770 obj=146378.28020 diff=0.28697\n",
      "iter=3 terr=0.33719 serr=1.00000 act=1254770 obj=122396.12890 diff=0.16384\n",
      "iter=4 terr=0.32156 serr=1.00000 act=1254770 obj=104707.41511 diff=0.14452\n",
      "iter=5 terr=0.32516 serr=0.99827 act=1254770 obj=88090.59756 diff=0.15870\n",
      "iter=6 terr=0.26296 serr=0.99394 act=1254770 obj=81164.39565 diff=0.07863\n",
      "iter=7 terr=0.25269 serr=0.89460 act=1254770 obj=76160.82351 diff=0.06165\n",
      "iter=8 terr=0.24218 serr=0.85071 act=1254770 obj=66900.21177 diff=0.12159\n",
      "iter=9 terr=0.27854 serr=0.82299 act=1254770 obj=58175.40155 diff=0.13042\n",
      "iter=10 terr=0.18795 serr=0.78978 act=1254770 obj=49066.21645 diff=0.15658\n",
      "iter=11 terr=0.17697 serr=0.78054 act=1254770 obj=46534.46829 diff=0.05160\n",
      "iter=12 terr=0.21071 serr=0.75801 act=1254770 obj=42507.15114 diff=0.08654\n",
      "iter=13 terr=0.18638 serr=0.73433 act=1254770 obj=38940.38745 diff=0.08391\n",
      "iter=14 terr=0.17687 serr=0.66070 act=1254770 obj=36643.97405 diff=0.05897\n",
      "iter=15 terr=0.17437 serr=0.61363 act=1254770 obj=32806.17728 diff=0.10473\n",
      "iter=16 terr=0.12575 serr=0.56483 act=1254770 obj=30829.27820 diff=0.06026\n",
      "iter=17 terr=0.14792 serr=0.57811 act=1254770 obj=29841.03112 diff=0.03206\n",
      "iter=18 terr=0.12846 serr=0.56310 act=1254770 obj=28248.79542 diff=0.05336\n",
      "iter=19 terr=0.12712 serr=0.55414 act=1254770 obj=26147.55952 diff=0.07438\n",
      "iter=20 terr=0.12670 serr=0.55732 act=1254770 obj=24974.35288 diff=0.04487\n",
      "iter=21 terr=0.12126 serr=0.52382 act=1254770 obj=23487.59503 diff=0.05953\n",
      "iter=22 terr=0.11617 serr=0.51632 act=1254770 obj=21763.35607 diff=0.07341\n",
      "iter=23 terr=0.12306 serr=0.49379 act=1254770 obj=21139.08129 diff=0.02868\n",
      "iter=24 terr=0.11584 serr=0.47387 act=1254770 obj=20199.10976 diff=0.04447\n",
      "iter=25 terr=0.10874 serr=0.45452 act=1254770 obj=18604.42383 diff=0.07895\n",
      "iter=26 terr=0.10787 serr=0.44326 act=1254770 obj=18007.07176 diff=0.03211\n",
      "iter=27 terr=0.10158 serr=0.43431 act=1254770 obj=17204.42153 diff=0.04457\n",
      "iter=28 terr=0.09064 serr=0.39734 act=1254770 obj=15834.89544 diff=0.07960\n",
      "iter=29 terr=0.12200 serr=0.42853 act=1254770 obj=17494.42202 diff=0.10480\n",
      "iter=30 terr=0.08950 serr=0.39215 act=1254770 obj=15347.80521 diff=0.12270\n",
      "iter=31 terr=0.08316 serr=0.37742 act=1254770 obj=14579.22005 diff=0.05008\n",
      "iter=32 terr=0.07696 serr=0.37309 act=1254770 obj=13378.27536 diff=0.08237\n",
      "iter=33 terr=0.07344 serr=0.35778 act=1254770 obj=12545.98038 diff=0.06221\n",
      "iter=34 terr=0.07675 serr=0.35461 act=1254770 obj=12418.57243 diff=0.01016\n",
      "iter=35 terr=0.06685 serr=0.33555 act=1254770 obj=10995.81816 diff=0.11457\n",
      "iter=36 terr=0.06591 serr=0.31967 act=1254770 obj=10760.75470 diff=0.02138\n",
      "iter=37 terr=0.06573 serr=0.31331 act=1254770 obj=10417.98489 diff=0.03185\n",
      "iter=38 terr=0.06722 serr=0.31735 act=1254770 obj=9959.19659 diff=0.04404\n",
      "iter=39 terr=0.06379 serr=0.31014 act=1254770 obj=9576.41568 diff=0.03843\n",
      "iter=40 terr=0.06131 serr=0.30234 act=1254770 obj=9263.32875 diff=0.03269\n",
      "iter=41 terr=0.05859 serr=0.29425 act=1254770 obj=8873.60504 diff=0.04207\n",
      "iter=42 terr=0.05911 serr=0.28848 act=1254770 obj=8603.69898 diff=0.03042\n",
      "iter=43 terr=0.05501 serr=0.27722 act=1254770 obj=8228.09146 diff=0.04366\n",
      "iter=44 terr=0.05426 serr=0.27288 act=1254770 obj=7894.25487 diff=0.04057\n",
      "iter=45 terr=0.05346 serr=0.26595 act=1254770 obj=7613.77045 diff=0.03553\n",
      "iter=46 terr=0.05411 serr=0.25036 act=1254770 obj=7180.32009 diff=0.05693\n",
      "iter=47 terr=0.05133 serr=0.25180 act=1254770 obj=6936.41890 diff=0.03397\n",
      "iter=48 terr=0.04684 serr=0.24372 act=1254770 obj=6658.83627 diff=0.04002\n",
      "iter=49 terr=0.04549 serr=0.24343 act=1254770 obj=6510.54898 diff=0.02227\n",
      "iter=50 terr=0.04466 serr=0.23737 act=1254770 obj=6164.12191 diff=0.05321\n",
      "iter=51 terr=0.04731 serr=0.23910 act=1254770 obj=6109.49560 diff=0.00886\n",
      "iter=52 terr=0.04471 serr=0.23563 act=1254770 obj=6003.78494 diff=0.01730\n",
      "iter=53 terr=0.04422 serr=0.23130 act=1254770 obj=5854.45087 diff=0.02487\n",
      "iter=54 terr=0.04410 serr=0.23101 act=1254770 obj=5787.82179 diff=0.01138\n",
      "iter=55 terr=0.04530 serr=0.22524 act=1254770 obj=5651.26273 diff=0.02359\n",
      "iter=56 terr=0.04328 serr=0.22524 act=1254770 obj=5530.28171 diff=0.02141\n",
      "iter=57 terr=0.04112 serr=0.21744 act=1254770 obj=5363.58171 diff=0.03014\n",
      "iter=58 terr=0.04009 serr=0.21022 act=1254770 obj=5175.55252 diff=0.03506\n",
      "iter=59 terr=0.03964 serr=0.20791 act=1254770 obj=5082.17637 diff=0.01804\n",
      "iter=60 terr=0.03952 serr=0.20300 act=1254770 obj=5040.77588 diff=0.00815\n",
      "iter=61 terr=0.03675 serr=0.19694 act=1254770 obj=4855.67880 diff=0.03672\n",
      "iter=62 terr=0.03618 serr=0.19521 act=1254770 obj=4763.68639 diff=0.01895\n",
      "iter=63 terr=0.03644 serr=0.19578 act=1254770 obj=4644.33521 diff=0.02505\n",
      "iter=64 terr=0.03513 serr=0.19232 act=1254770 obj=4438.05971 diff=0.04441\n",
      "iter=65 terr=0.06308 serr=0.23419 act=1254770 obj=8017.98429 diff=0.80664\n",
      "iter=66 terr=0.03652 serr=0.18885 act=1254770 obj=4376.17982 diff=0.45420\n",
      "iter=67 terr=0.03257 serr=0.18106 act=1254770 obj=4166.86544 diff=0.04783\n",
      "iter=68 terr=0.03064 serr=0.17528 act=1254770 obj=4034.05170 diff=0.03187\n",
      "iter=69 terr=0.03136 serr=0.17470 act=1254770 obj=3963.92118 diff=0.01738\n",
      "iter=70 terr=0.02916 serr=0.17008 act=1254770 obj=3897.53369 diff=0.01675\n",
      "iter=71 terr=0.02855 serr=0.16662 act=1254770 obj=3864.78101 diff=0.00840\n",
      "iter=72 terr=0.02942 serr=0.16691 act=1254770 obj=3816.65408 diff=0.01245\n",
      "iter=73 terr=0.02847 serr=0.16517 act=1254770 obj=3749.97510 diff=0.01747\n",
      "iter=74 terr=0.02716 serr=0.16171 act=1254770 obj=3545.95117 diff=0.05441\n",
      "iter=75 terr=0.02665 serr=0.15824 act=1254770 obj=3457.38764 diff=0.02498\n",
      "iter=76 terr=0.05777 serr=0.21369 act=1254770 obj=4828.26075 diff=0.39651\n",
      "iter=77 terr=0.02667 serr=0.15940 act=1254770 obj=3416.60332 diff=0.29237\n",
      "iter=78 terr=0.02488 serr=0.14871 act=1254770 obj=3256.84794 diff=0.04676\n",
      "iter=79 terr=0.02319 serr=0.14034 act=1254770 obj=3091.66929 diff=0.05072\n",
      "iter=80 terr=0.02745 serr=0.14785 act=1254770 obj=3186.68264 diff=0.03073\n",
      "iter=81 terr=0.02223 serr=0.13890 act=1254770 obj=3023.31444 diff=0.05127\n",
      "iter=82 terr=0.02159 serr=0.13428 act=1254770 obj=2918.73190 diff=0.03459\n",
      "iter=83 terr=0.02107 serr=0.13110 act=1254770 obj=2844.62140 diff=0.02539\n",
      "iter=84 terr=0.02860 serr=0.14958 act=1254770 obj=2999.59520 diff=0.05448\n",
      "iter=85 terr=0.02172 serr=0.13139 act=1254770 obj=2803.26500 diff=0.06545\n",
      "iter=86 terr=0.01983 serr=0.12532 act=1254770 obj=2709.30222 diff=0.03352\n",
      "iter=87 terr=0.01983 serr=0.12244 act=1254770 obj=2591.87838 diff=0.04334\n",
      "iter=88 terr=0.01904 serr=0.11897 act=1254770 obj=2515.66698 diff=0.02940\n",
      "iter=89 terr=0.01798 serr=0.11926 act=1254770 obj=2411.18558 diff=0.04153\n",
      "iter=90 terr=0.01730 serr=0.11955 act=1254770 obj=2346.93795 diff=0.02665\n",
      "iter=91 terr=0.01673 serr=0.12070 act=1254770 obj=2300.52937 diff=0.01977\n",
      "iter=92 terr=0.01627 serr=0.11753 act=1254770 obj=2263.94852 diff=0.01590\n",
      "iter=93 terr=0.01656 serr=0.11724 act=1254770 obj=2234.24340 diff=0.01312\n",
      "iter=94 terr=0.01541 serr=0.11204 act=1254770 obj=2160.10436 diff=0.03318\n",
      "iter=95 terr=0.01467 serr=0.10944 act=1254770 obj=2102.37860 diff=0.02672\n",
      "iter=96 terr=0.01393 serr=0.10684 act=1254770 obj=2073.84991 diff=0.01357\n",
      "iter=97 terr=0.01318 serr=0.10280 act=1254770 obj=2031.62379 diff=0.02036\n",
      "iter=98 terr=0.01296 serr=0.09818 act=1254770 obj=1964.95689 diff=0.03281\n",
      "iter=99 terr=0.01387 serr=0.09414 act=1254770 obj=1903.33306 diff=0.03136\n",
      "iter=100 terr=0.01132 serr=0.08750 act=1254770 obj=1805.36618 diff=0.05147\n",
      "iter=101 terr=0.01158 serr=0.08894 act=1254770 obj=1773.45152 diff=0.01768\n",
      "iter=102 terr=0.01109 serr=0.08750 act=1254770 obj=1752.40507 diff=0.01187\n",
      "iter=103 terr=0.01139 serr=0.08750 act=1254770 obj=1723.62964 diff=0.01642\n",
      "iter=104 terr=0.01169 serr=0.08490 act=1254770 obj=1662.86232 diff=0.03526\n",
      "iter=105 terr=0.01706 serr=0.09385 act=1254770 obj=1815.14784 diff=0.09158\n",
      "iter=106 terr=0.01109 serr=0.08316 act=1254770 obj=1639.81817 diff=0.09659\n",
      "iter=107 terr=0.01034 serr=0.07912 act=1254770 obj=1616.85612 diff=0.01400\n",
      "iter=108 terr=0.00997 serr=0.07681 act=1254770 obj=1595.10848 diff=0.01345\n",
      "iter=109 terr=0.00976 serr=0.07566 act=1254770 obj=1555.89676 diff=0.02458\n",
      "iter=110 terr=0.00939 serr=0.07364 act=1254770 obj=1512.48787 diff=0.02790\n",
      "iter=111 terr=0.03298 serr=0.13052 act=1254770 obj=2472.30227 diff=0.63459\n",
      "iter=112 terr=0.00971 serr=0.07277 act=1254770 obj=1505.80485 diff=0.39093\n",
      "iter=113 terr=0.00932 serr=0.06988 act=1254770 obj=1475.29940 diff=0.02026\n",
      "iter=114 terr=0.00918 serr=0.06757 act=1254770 obj=1445.38219 diff=0.02028\n",
      "iter=115 terr=0.00793 serr=0.06295 act=1254770 obj=1408.56980 diff=0.02547\n",
      "iter=116 terr=0.00751 serr=0.06151 act=1254770 obj=1400.10914 diff=0.00601\n",
      "iter=117 terr=0.00734 serr=0.06180 act=1254770 obj=1390.73472 diff=0.00670\n",
      "iter=118 terr=0.00779 serr=0.06266 act=1254770 obj=1342.70516 diff=0.03454\n",
      "iter=119 terr=0.00818 serr=0.06151 act=1254770 obj=1331.07795 diff=0.00866\n",
      "iter=120 terr=0.00732 serr=0.06064 act=1254770 obj=1309.76274 diff=0.01601\n",
      "iter=121 terr=0.00727 serr=0.05949 act=1254770 obj=1255.71947 diff=0.04126\n",
      "iter=122 terr=0.00571 serr=0.05140 act=1254770 obj=1203.13809 diff=0.04187\n",
      "iter=123 terr=0.00538 serr=0.05053 act=1254770 obj=1194.24854 diff=0.00739\n",
      "iter=124 terr=0.00521 serr=0.04909 act=1254770 obj=1184.66851 diff=0.00802\n",
      "iter=125 terr=0.00511 serr=0.04794 act=1254770 obj=1159.03954 diff=0.02163\n",
      "iter=126 terr=0.00485 serr=0.04649 act=1254770 obj=1144.92868 diff=0.01217\n",
      "iter=127 terr=0.00477 serr=0.04476 act=1254770 obj=1134.90692 diff=0.00875\n",
      "iter=128 terr=0.00467 serr=0.04418 act=1254770 obj=1122.95448 diff=0.01053\n",
      "iter=129 terr=0.00412 serr=0.04216 act=1254770 obj=1099.99693 diff=0.02044\n",
      "iter=130 terr=0.00400 serr=0.04072 act=1254770 obj=1084.58022 diff=0.01402\n",
      "iter=131 terr=0.00366 serr=0.03898 act=1254770 obj=1062.78647 diff=0.02009\n",
      "iter=132 terr=0.00354 serr=0.03638 act=1254770 obj=1041.30070 diff=0.02022\n",
      "iter=133 terr=0.00377 serr=0.03523 act=1254770 obj=1015.47367 diff=0.02480\n",
      "iter=134 terr=0.00356 serr=0.03379 act=1254770 obj=977.75860 diff=0.03714\n",
      "iter=135 terr=0.00336 serr=0.03148 act=1254770 obj=949.83297 diff=0.02856\n",
      "iter=136 terr=0.00344 serr=0.03263 act=1254770 obj=926.61783 diff=0.02444\n",
      "iter=137 terr=0.00321 serr=0.03176 act=1254770 obj=907.85888 diff=0.02024\n",
      "iter=138 terr=0.00287 serr=0.03003 act=1254770 obj=880.86116 diff=0.02974\n",
      "iter=139 terr=0.00258 serr=0.02917 act=1254770 obj=860.24961 diff=0.02340\n",
      "iter=140 terr=0.00286 serr=0.02772 act=1254770 obj=863.17783 diff=0.00340\n",
      "iter=141 terr=0.00227 serr=0.02628 act=1254770 obj=848.04320 diff=0.01753\n",
      "iter=142 terr=0.00231 serr=0.02657 act=1254770 obj=837.90858 diff=0.01195\n",
      "iter=143 terr=0.00208 serr=0.02397 act=1254770 obj=828.80273 diff=0.01087\n",
      "iter=144 terr=0.00190 serr=0.02252 act=1254770 obj=823.76682 diff=0.00608\n",
      "iter=145 terr=0.00189 serr=0.02224 act=1254770 obj=809.48138 diff=0.01734\n",
      "iter=146 terr=0.00190 serr=0.02166 act=1254770 obj=790.32428 diff=0.02367\n",
      "iter=147 terr=0.00145 serr=0.01617 act=1254770 obj=765.29498 diff=0.03167\n",
      "iter=148 terr=0.00133 serr=0.01473 act=1254770 obj=731.14491 diff=0.04462\n",
      "iter=149 terr=0.00156 serr=0.01588 act=1254770 obj=713.03405 diff=0.02477\n",
      "iter=150 terr=0.00115 serr=0.01299 act=1254770 obj=696.58318 diff=0.02307\n",
      "iter=151 terr=0.00104 serr=0.01271 act=1254770 obj=691.34212 diff=0.00752\n",
      "iter=152 terr=0.00099 serr=0.01299 act=1254770 obj=675.38987 diff=0.02307\n",
      "iter=153 terr=0.00120 serr=0.01530 act=1254770 obj=675.81945 diff=0.00064\n",
      "iter=154 terr=0.00098 serr=0.01328 act=1254770 obj=669.04235 diff=0.01003\n",
      "iter=155 terr=0.00102 serr=0.01357 act=1254770 obj=660.63309 diff=0.01257\n",
      "iter=156 terr=0.00102 serr=0.01357 act=1254770 obj=655.28612 diff=0.00809\n",
      "iter=157 terr=0.00102 serr=0.01299 act=1254770 obj=644.65096 diff=0.01623\n",
      "iter=158 terr=0.00104 serr=0.01213 act=1254770 obj=636.33182 diff=0.01290\n",
      "iter=159 terr=0.00100 serr=0.01184 act=1254770 obj=621.68304 diff=0.02302\n",
      "iter=160 terr=0.00079 serr=0.01040 act=1254770 obj=610.13415 diff=0.01858\n",
      "iter=161 terr=0.00078 serr=0.01011 act=1254770 obj=604.83935 diff=0.00868\n",
      "iter=162 terr=0.00079 serr=0.01011 act=1254770 obj=608.50809 diff=0.00607\n",
      "iter=163 terr=0.00075 serr=0.00953 act=1254770 obj=601.30675 diff=0.01183\n",
      "iter=164 terr=0.00075 serr=0.00953 act=1254770 obj=595.57292 diff=0.00954\n",
      "iter=165 terr=0.00066 serr=0.00866 act=1254770 obj=576.61235 diff=0.03184\n",
      "iter=166 terr=0.00067 serr=0.00866 act=1254770 obj=570.43989 diff=0.01070\n",
      "iter=167 terr=0.00056 serr=0.00780 act=1254770 obj=560.82936 diff=0.01685\n",
      "iter=168 terr=0.00060 serr=0.00809 act=1254770 obj=557.39948 diff=0.00612\n",
      "iter=169 terr=0.00060 serr=0.00809 act=1254770 obj=554.34252 diff=0.00548\n",
      "iter=170 terr=0.00058 serr=0.00780 act=1254770 obj=549.91805 diff=0.00798\n",
      "iter=171 terr=0.00058 serr=0.00866 act=1254770 obj=550.02427 diff=0.00019\n",
      "iter=172 terr=0.00058 serr=0.00809 act=1254770 obj=545.64225 diff=0.00797\n",
      "iter=173 terr=0.00056 serr=0.00809 act=1254770 obj=540.90585 diff=0.00868\n",
      "iter=174 terr=0.00041 serr=0.00606 act=1254770 obj=534.85665 diff=0.01118\n",
      "iter=175 terr=0.00044 serr=0.00635 act=1254770 obj=533.65106 diff=0.00225\n",
      "iter=176 terr=0.00044 serr=0.00606 act=1254770 obj=531.70795 diff=0.00364\n",
      "iter=177 terr=0.00037 serr=0.00549 act=1254770 obj=530.68935 diff=0.00192\n",
      "iter=178 terr=0.00037 serr=0.00549 act=1254770 obj=527.89820 diff=0.00526\n",
      "iter=179 terr=0.00037 serr=0.00549 act=1254770 obj=525.15049 diff=0.00520\n",
      "iter=180 terr=0.00034 serr=0.00520 act=1254770 obj=519.27239 diff=0.01119\n",
      "iter=181 terr=0.00034 serr=0.00520 act=1254770 obj=511.29730 diff=0.01536\n",
      "iter=182 terr=0.00031 serr=0.00462 act=1254770 obj=506.82315 diff=0.00875\n",
      "iter=183 terr=0.00027 serr=0.00433 act=1254770 obj=499.63285 diff=0.01419\n",
      "iter=184 terr=0.00042 serr=0.00404 act=1254770 obj=529.69181 diff=0.06016\n",
      "iter=185 terr=0.00024 serr=0.00375 act=1254770 obj=497.94790 diff=0.05993\n",
      "iter=186 terr=0.00026 serr=0.00375 act=1254770 obj=494.14196 diff=0.00764\n",
      "iter=187 terr=0.00025 serr=0.00347 act=1254770 obj=490.76341 diff=0.00684\n",
      "iter=188 terr=0.00023 serr=0.00318 act=1254770 obj=487.92570 diff=0.00578\n",
      "iter=189 terr=0.00023 serr=0.00318 act=1254770 obj=484.27088 diff=0.00749\n",
      "iter=190 terr=0.00023 serr=0.00289 act=1254770 obj=481.67229 diff=0.00537\n",
      "iter=191 terr=0.00023 serr=0.00289 act=1254770 obj=478.95298 diff=0.00565\n",
      "iter=192 terr=0.00023 serr=0.00289 act=1254770 obj=477.27894 diff=0.00350\n",
      "iter=193 terr=0.00023 serr=0.00289 act=1254770 obj=475.71998 diff=0.00327\n",
      "iter=194 terr=0.00019 serr=0.00202 act=1254770 obj=472.70308 diff=0.00634\n",
      "iter=195 terr=0.00019 serr=0.00202 act=1254770 obj=468.74060 diff=0.00838\n",
      "iter=196 terr=0.00030 serr=0.00318 act=1254770 obj=471.01085 diff=0.00484\n",
      "iter=197 terr=0.00019 serr=0.00202 act=1254770 obj=466.83442 diff=0.00887\n",
      "iter=198 terr=0.00019 serr=0.00231 act=1254770 obj=464.99998 diff=0.00393\n",
      "iter=199 terr=0.00020 serr=0.00260 act=1254770 obj=463.68518 diff=0.00283\n",
      "iter=200 terr=0.00020 serr=0.00260 act=1254770 obj=461.24290 diff=0.00527\n",
      "iter=201 terr=0.00020 serr=0.00231 act=1254770 obj=463.14107 diff=0.00412\n",
      "iter=202 terr=0.00020 serr=0.00260 act=1254770 obj=458.38494 diff=0.01027\n",
      "iter=203 terr=0.00020 serr=0.00260 act=1254770 obj=452.78121 diff=0.01222\n",
      "iter=204 terr=0.00021 serr=0.00289 act=1254770 obj=447.60132 diff=0.01144\n",
      "iter=205 terr=0.00021 serr=0.00289 act=1254770 obj=443.88395 diff=0.00831\n",
      "iter=206 terr=0.00021 serr=0.00289 act=1254770 obj=445.05297 diff=0.00263\n",
      "iter=207 terr=0.00021 serr=0.00289 act=1254770 obj=442.04891 diff=0.00675\n",
      "iter=208 terr=0.00021 serr=0.00289 act=1254770 obj=440.66758 diff=0.00312\n",
      "iter=209 terr=0.00021 serr=0.00289 act=1254770 obj=439.80284 diff=0.00196\n",
      "iter=210 terr=0.00020 serr=0.00260 act=1254770 obj=438.19070 diff=0.00367\n",
      "iter=211 terr=0.00019 serr=0.00231 act=1254770 obj=438.40576 diff=0.00049\n",
      "iter=212 terr=0.00020 serr=0.00260 act=1254770 obj=436.68600 diff=0.00392\n",
      "iter=213 terr=0.00019 serr=0.00231 act=1254770 obj=433.66416 diff=0.00692\n",
      "iter=214 terr=0.00019 serr=0.00231 act=1254770 obj=430.59309 diff=0.00708\n",
      "iter=215 terr=0.00019 serr=0.00231 act=1254770 obj=428.87467 diff=0.00399\n",
      "iter=216 terr=0.00019 serr=0.00231 act=1254770 obj=427.91990 diff=0.00223\n",
      "iter=217 terr=0.00019 serr=0.00260 act=1254770 obj=426.91983 diff=0.00234\n",
      "iter=218 terr=0.00019 serr=0.00231 act=1254770 obj=425.56758 diff=0.00317\n",
      "iter=219 terr=0.00019 serr=0.00260 act=1254770 obj=424.53628 diff=0.00242\n",
      "iter=220 terr=0.00019 serr=0.00231 act=1254770 obj=423.88257 diff=0.00154\n",
      "iter=221 terr=0.00019 serr=0.00231 act=1254770 obj=422.91382 diff=0.00229\n",
      "iter=222 terr=0.00019 serr=0.00202 act=1254770 obj=421.58732 diff=0.00314\n",
      "iter=223 terr=0.00018 serr=0.00202 act=1254770 obj=420.78160 diff=0.00191\n",
      "iter=224 terr=0.00018 serr=0.00202 act=1254770 obj=420.04815 diff=0.00174\n",
      "iter=225 terr=0.00018 serr=0.00202 act=1254770 obj=419.65969 diff=0.00092\n",
      "iter=226 terr=0.00018 serr=0.00202 act=1254770 obj=419.40491 diff=0.00061\n",
      "iter=227 terr=0.00018 serr=0.00202 act=1254770 obj=418.72819 diff=0.00161\n",
      "iter=228 terr=0.00018 serr=0.00173 act=1254770 obj=417.60400 diff=0.00268\n",
      "iter=229 terr=0.00018 serr=0.00173 act=1254770 obj=416.11302 diff=0.00357\n",
      "iter=230 terr=0.00018 serr=0.00144 act=1254770 obj=414.74205 diff=0.00329\n",
      "iter=231 terr=0.00018 serr=0.00173 act=1254770 obj=413.19478 diff=0.00373\n",
      "iter=232 terr=0.00018 serr=0.00173 act=1254770 obj=412.94136 diff=0.00061\n",
      "iter=233 terr=0.00018 serr=0.00173 act=1254770 obj=412.64736 diff=0.00071\n",
      "iter=234 terr=0.00018 serr=0.00173 act=1254770 obj=412.02665 diff=0.00150\n",
      "iter=235 terr=0.00018 serr=0.00173 act=1254770 obj=410.92200 diff=0.00268\n",
      "iter=236 terr=0.00018 serr=0.00173 act=1254770 obj=409.86589 diff=0.00257\n",
      "iter=237 terr=0.00018 serr=0.00173 act=1254770 obj=412.73595 diff=0.00700\n",
      "iter=238 terr=0.00018 serr=0.00202 act=1254770 obj=409.30347 diff=0.00832\n",
      "iter=239 terr=0.00018 serr=0.00202 act=1254770 obj=408.28770 diff=0.00248\n",
      "iter=240 terr=0.00018 serr=0.00202 act=1254770 obj=407.87042 diff=0.00102\n",
      "iter=241 terr=0.00018 serr=0.00173 act=1254770 obj=405.55594 diff=0.00567\n",
      "iter=242 terr=0.00018 serr=0.00202 act=1254770 obj=404.47783 diff=0.00266\n",
      "iter=243 terr=0.00018 serr=0.00202 act=1254770 obj=403.64692 diff=0.00205\n",
      "iter=244 terr=0.00018 serr=0.00202 act=1254770 obj=403.00201 diff=0.00160\n",
      "iter=245 terr=0.00018 serr=0.00202 act=1254770 obj=402.46156 diff=0.00134\n",
      "iter=246 terr=0.00018 serr=0.00173 act=1254770 obj=401.98548 diff=0.00118\n",
      "iter=247 terr=0.00018 serr=0.00144 act=1254770 obj=401.23950 diff=0.00186\n",
      "iter=248 terr=0.00018 serr=0.00144 act=1254770 obj=400.49659 diff=0.00185\n",
      "iter=249 terr=0.00018 serr=0.00173 act=1254770 obj=399.84143 diff=0.00164\n",
      "iter=250 terr=0.00018 serr=0.00173 act=1254770 obj=399.58123 diff=0.00065\n",
      "iter=251 terr=0.00018 serr=0.00173 act=1254770 obj=399.18798 diff=0.00098\n",
      "iter=252 terr=0.00018 serr=0.00173 act=1254770 obj=399.07780 diff=0.00028\n",
      "iter=253 terr=0.00018 serr=0.00202 act=1254770 obj=398.67717 diff=0.00100\n",
      "iter=254 terr=0.00018 serr=0.00173 act=1254770 obj=398.23273 diff=0.00111\n",
      "iter=255 terr=0.00018 serr=0.00173 act=1254770 obj=397.95653 diff=0.00069\n",
      "iter=256 terr=0.00018 serr=0.00173 act=1254770 obj=397.70798 diff=0.00062\n",
      "iter=257 terr=0.00018 serr=0.00173 act=1254770 obj=397.57292 diff=0.00034\n",
      "iter=258 terr=0.00018 serr=0.00173 act=1254770 obj=397.41950 diff=0.00039\n",
      "iter=259 terr=0.00018 serr=0.00173 act=1254770 obj=396.96676 diff=0.00114\n",
      "iter=260 terr=0.00018 serr=0.00202 act=1254770 obj=396.47918 diff=0.00123\n",
      "iter=261 terr=0.00018 serr=0.00202 act=1254770 obj=396.11982 diff=0.00091\n",
      "iter=262 terr=0.00018 serr=0.00202 act=1254770 obj=395.84513 diff=0.00069\n",
      "iter=263 terr=0.00018 serr=0.00144 act=1254770 obj=395.25524 diff=0.00149\n",
      "iter=264 terr=0.00018 serr=0.00144 act=1254770 obj=395.45449 diff=0.00050\n",
      "iter=265 terr=0.00018 serr=0.00173 act=1254770 obj=394.91981 diff=0.00135\n",
      "iter=266 terr=0.00018 serr=0.00173 act=1254770 obj=394.62972 diff=0.00073\n",
      "iter=267 terr=0.00018 serr=0.00173 act=1254770 obj=393.50703 diff=0.00284\n",
      "iter=268 terr=0.00018 serr=0.00173 act=1254770 obj=393.06120 diff=0.00113\n",
      "iter=269 terr=0.00018 serr=0.00173 act=1254770 obj=392.62715 diff=0.00110\n",
      "iter=270 terr=0.00018 serr=0.00173 act=1254770 obj=392.03818 diff=0.00150\n",
      "iter=271 terr=0.00018 serr=0.00173 act=1254770 obj=391.71397 diff=0.00083\n",
      "iter=272 terr=0.00018 serr=0.00144 act=1254770 obj=392.08043 diff=0.00094\n",
      "iter=273 terr=0.00018 serr=0.00173 act=1254770 obj=391.44748 diff=0.00161\n",
      "iter=274 terr=0.00018 serr=0.00173 act=1254770 obj=390.98335 diff=0.00119\n",
      "iter=275 terr=0.00018 serr=0.00173 act=1254770 obj=390.63703 diff=0.00089\n",
      "iter=276 terr=0.00018 serr=0.00144 act=1254770 obj=389.55939 diff=0.00276\n",
      "iter=277 terr=0.00018 serr=0.00144 act=1254770 obj=389.03207 diff=0.00135\n",
      "iter=278 terr=0.00018 serr=0.00173 act=1254770 obj=389.00228 diff=0.00008\n",
      "iter=279 terr=0.00018 serr=0.00173 act=1254770 obj=388.74003 diff=0.00067\n",
      "iter=280 terr=0.00018 serr=0.00202 act=1254770 obj=388.46686 diff=0.00070\n",
      "iter=281 terr=0.00018 serr=0.00202 act=1254770 obj=388.28030 diff=0.00048\n",
      "iter=282 terr=0.00018 serr=0.00202 act=1254770 obj=388.07815 diff=0.00052\n",
      "iter=283 terr=0.00018 serr=0.00202 act=1254770 obj=385.37827 diff=0.00696\n",
      "iter=284 terr=0.00018 serr=0.00202 act=1254770 obj=384.30643 diff=0.00278\n",
      "iter=285 terr=0.00018 serr=0.00202 act=1254770 obj=383.68901 diff=0.00161\n",
      "iter=286 terr=0.00018 serr=0.00173 act=1254770 obj=383.03770 diff=0.00170\n",
      "iter=287 terr=0.00018 serr=0.00173 act=1254770 obj=382.75090 diff=0.00075\n",
      "iter=288 terr=0.00018 serr=0.00173 act=1254770 obj=382.31924 diff=0.00113\n",
      "iter=289 terr=0.00018 serr=0.00144 act=1254770 obj=382.27836 diff=0.00011\n",
      "iter=290 terr=0.00018 serr=0.00144 act=1254770 obj=381.85593 diff=0.00111\n",
      "iter=291 terr=0.00018 serr=0.00144 act=1254770 obj=381.27979 diff=0.00151\n",
      "iter=292 terr=0.00018 serr=0.00173 act=1254770 obj=380.75355 diff=0.00138\n",
      "iter=293 terr=0.00018 serr=0.00173 act=1254770 obj=380.32280 diff=0.00113\n",
      "iter=294 terr=0.00018 serr=0.00173 act=1254770 obj=379.20787 diff=0.00293\n",
      "iter=295 terr=0.00018 serr=0.00173 act=1254770 obj=388.23404 diff=0.02380\n",
      "iter=296 terr=0.00018 serr=0.00173 act=1254770 obj=379.01106 diff=0.02376\n",
      "iter=297 terr=0.00018 serr=0.00173 act=1254770 obj=378.61112 diff=0.00106\n",
      "iter=298 terr=0.00018 serr=0.00202 act=1254770 obj=378.43172 diff=0.00047\n",
      "iter=299 terr=0.00018 serr=0.00202 act=1254770 obj=378.26120 diff=0.00045\n",
      "iter=300 terr=0.00018 serr=0.00202 act=1254770 obj=378.03412 diff=0.00060\n",
      "iter=301 terr=0.00018 serr=0.00202 act=1254770 obj=377.77420 diff=0.00069\n",
      "iter=302 terr=0.00018 serr=0.00202 act=1254770 obj=377.54870 diff=0.00060\n",
      "iter=303 terr=0.00018 serr=0.00202 act=1254770 obj=377.11209 diff=0.00116\n",
      "iter=304 terr=0.00018 serr=0.00202 act=1254770 obj=377.32003 diff=0.00055\n",
      "iter=305 terr=0.00018 serr=0.00173 act=1254770 obj=376.81950 diff=0.00133\n",
      "iter=306 terr=0.00018 serr=0.00173 act=1254770 obj=376.57980 diff=0.00064\n",
      "iter=307 terr=0.00018 serr=0.00173 act=1254770 obj=376.31151 diff=0.00071\n",
      "iter=308 terr=0.00018 serr=0.00173 act=1254770 obj=376.27206 diff=0.00010\n",
      "iter=309 terr=0.00018 serr=0.00173 act=1254770 obj=375.96968 diff=0.00080\n",
      "iter=310 terr=0.00018 serr=0.00173 act=1254770 obj=375.72189 diff=0.00066\n",
      "iter=311 terr=0.00018 serr=0.00202 act=1254770 obj=375.49652 diff=0.00060\n",
      "iter=312 terr=0.00018 serr=0.00202 act=1254770 obj=375.36979 diff=0.00034\n",
      "iter=313 terr=0.00018 serr=0.00173 act=1254770 obj=375.11780 diff=0.00067\n",
      "iter=314 terr=0.00018 serr=0.00202 act=1254770 obj=374.92102 diff=0.00052\n",
      "iter=315 terr=0.00018 serr=0.00173 act=1254770 obj=374.84572 diff=0.00020\n",
      "iter=316 terr=0.00018 serr=0.00173 act=1254770 obj=374.77546 diff=0.00019\n",
      "iter=317 terr=0.00018 serr=0.00144 act=1254770 obj=374.64027 diff=0.00036\n",
      "iter=318 terr=0.00018 serr=0.00173 act=1254770 obj=374.37213 diff=0.00072\n",
      "iter=319 terr=0.00018 serr=0.00173 act=1254770 obj=374.17963 diff=0.00051\n",
      "iter=320 terr=0.00018 serr=0.00173 act=1254770 obj=374.00672 diff=0.00046\n",
      "iter=321 terr=0.00018 serr=0.00202 act=1254770 obj=373.89106 diff=0.00031\n",
      "iter=322 terr=0.00018 serr=0.00202 act=1254770 obj=373.74050 diff=0.00040\n",
      "iter=323 terr=0.00018 serr=0.00202 act=1254770 obj=373.53333 diff=0.00055\n",
      "iter=324 terr=0.00018 serr=0.00173 act=1254770 obj=373.27179 diff=0.00070\n",
      "iter=325 terr=0.00018 serr=0.00173 act=1254770 obj=373.27503 diff=0.00001\n",
      "iter=326 terr=0.00018 serr=0.00144 act=1254770 obj=373.12138 diff=0.00041\n",
      "iter=327 terr=0.00018 serr=0.00173 act=1254770 obj=372.95730 diff=0.00044\n",
      "iter=328 terr=0.00018 serr=0.00173 act=1254770 obj=372.84708 diff=0.00030\n",
      "iter=329 terr=0.00018 serr=0.00173 act=1254770 obj=372.66065 diff=0.00050\n",
      "iter=330 terr=0.00018 serr=0.00202 act=1254770 obj=372.38471 diff=0.00074\n",
      "iter=331 terr=0.00018 serr=0.00202 act=1254770 obj=372.68193 diff=0.00080\n",
      "iter=332 terr=0.00018 serr=0.00202 act=1254770 obj=372.29731 diff=0.00103\n",
      "iter=333 terr=0.00018 serr=0.00202 act=1254770 obj=372.15001 diff=0.00040\n",
      "iter=334 terr=0.00018 serr=0.00202 act=1254770 obj=372.04656 diff=0.00028\n",
      "iter=335 terr=0.00018 serr=0.00173 act=1254770 obj=372.00687 diff=0.00011\n",
      "iter=336 terr=0.00018 serr=0.00173 act=1254770 obj=371.96663 diff=0.00011\n",
      "iter=337 terr=0.00018 serr=0.00202 act=1254770 obj=371.46634 diff=0.00134\n",
      "iter=338 terr=0.00018 serr=0.00173 act=1254770 obj=371.06384 diff=0.00108\n",
      "iter=339 terr=0.00018 serr=0.00173 act=1254770 obj=373.44964 diff=0.00643\n",
      "iter=340 terr=0.00018 serr=0.00173 act=1254770 obj=370.97030 diff=0.00664\n",
      "iter=341 terr=0.00018 serr=0.00173 act=1254770 obj=370.80766 diff=0.00044\n",
      "iter=342 terr=0.00018 serr=0.00173 act=1254770 obj=370.78371 diff=0.00006\n",
      "iter=343 terr=0.00018 serr=0.00173 act=1254770 obj=370.74532 diff=0.00010\n",
      "iter=344 terr=0.00018 serr=0.00173 act=1254770 obj=370.71730 diff=0.00008\n",
      "iter=345 terr=0.00018 serr=0.00173 act=1254770 obj=370.66060 diff=0.00015\n",
      "iter=346 terr=0.00018 serr=0.00202 act=1254770 obj=370.52299 diff=0.00037\n",
      "iter=347 terr=0.00018 serr=0.00202 act=1254770 obj=370.41900 diff=0.00028\n",
      "iter=348 terr=0.00018 serr=0.00202 act=1254770 obj=370.28012 diff=0.00037\n",
      "iter=349 terr=0.00018 serr=0.00202 act=1254770 obj=370.23915 diff=0.00011\n",
      "iter=350 terr=0.00018 serr=0.00202 act=1254770 obj=370.17044 diff=0.00019\n",
      "iter=351 terr=0.00018 serr=0.00202 act=1254770 obj=370.08068 diff=0.00024\n",
      "iter=352 terr=0.00018 serr=0.00202 act=1254770 obj=369.83541 diff=0.00066\n",
      "iter=353 terr=0.00018 serr=0.00173 act=1254770 obj=369.68538 diff=0.00041\n",
      "iter=354 terr=0.00018 serr=0.00144 act=1254770 obj=369.57129 diff=0.00031\n",
      "iter=355 terr=0.00018 serr=0.00173 act=1254770 obj=369.61671 diff=0.00012\n",
      "iter=356 terr=0.00018 serr=0.00144 act=1254770 obj=369.50426 diff=0.00030\n",
      "iter=357 terr=0.00018 serr=0.00144 act=1254770 obj=369.41734 diff=0.00024\n",
      "iter=358 terr=0.00018 serr=0.00144 act=1254770 obj=369.30929 diff=0.00029\n",
      "iter=359 terr=0.00018 serr=0.00173 act=1254770 obj=369.25863 diff=0.00014\n",
      "iter=360 terr=0.00018 serr=0.00202 act=1254770 obj=369.23072 diff=0.00008\n",
      "iter=361 terr=0.00018 serr=0.00202 act=1254770 obj=369.18648 diff=0.00012\n",
      "iter=362 terr=0.00018 serr=0.00173 act=1254770 obj=369.07639 diff=0.00030\n",
      "iter=363 terr=0.00018 serr=0.00144 act=1254770 obj=370.14605 diff=0.00290\n",
      "iter=364 terr=0.00018 serr=0.00144 act=1254770 obj=369.05552 diff=0.00295\n",
      "iter=365 terr=0.00018 serr=0.00173 act=1254770 obj=368.99238 diff=0.00017\n",
      "iter=366 terr=0.00018 serr=0.00173 act=1254770 obj=368.92547 diff=0.00018\n",
      "iter=367 terr=0.00018 serr=0.00173 act=1254770 obj=368.86880 diff=0.00015\n",
      "iter=368 terr=0.00018 serr=0.00144 act=1254770 obj=368.81962 diff=0.00013\n",
      "iter=369 terr=0.00018 serr=0.00144 act=1254770 obj=368.79118 diff=0.00008\n",
      "iter=370 terr=0.00018 serr=0.00173 act=1254770 obj=368.73019 diff=0.00017\n",
      "iter=371 terr=0.00018 serr=0.00173 act=1254770 obj=368.66967 diff=0.00016\n",
      "iter=372 terr=0.00018 serr=0.00173 act=1254770 obj=368.60444 diff=0.00018\n",
      "iter=373 terr=0.00018 serr=0.00173 act=1254770 obj=368.50779 diff=0.00026\n",
      "iter=374 terr=0.00018 serr=0.00173 act=1254770 obj=368.45162 diff=0.00015\n",
      "iter=375 terr=0.00018 serr=0.00173 act=1254770 obj=368.41136 diff=0.00011\n",
      "iter=376 terr=0.00018 serr=0.00173 act=1254770 obj=368.35711 diff=0.00015\n",
      "iter=377 terr=0.00018 serr=0.00173 act=1254770 obj=368.33264 diff=0.00007\n",
      "iter=378 terr=0.00018 serr=0.00173 act=1254770 obj=368.29647 diff=0.00010\n",
      "iter=379 terr=0.00018 serr=0.00173 act=1254770 obj=368.25976 diff=0.00010\n",
      "\n",
      "Done!2354.87 s\n",
      "\n",
      "\n",
      "2019-06-20 20:25:26.403685 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-20 20:25:34.054235 200\n",
      "2019-06-20 20:25:41.800321 400\n",
      "2019-06-20 20:25:49.584570 600\n",
      "2019-06-20 20:25:57.297878 800\n",
      "2019-06-20 20:26:04.847467 1000\n",
      "\n",
      "The Final Performance is:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anno</th>\n",
       "      <th>Pred</th>\n",
       "      <th>Match</th>\n",
       "      <th>R</th>\n",
       "      <th>P</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CONT</th>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDU</th>\n",
       "      <td>263</td>\n",
       "      <td>267</td>\n",
       "      <td>250</td>\n",
       "      <td>0.950570</td>\n",
       "      <td>0.936330</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME</th>\n",
       "      <td>302</td>\n",
       "      <td>295</td>\n",
       "      <td>294</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.996610</td>\n",
       "      <td>0.984925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>1421</td>\n",
       "      <td>1411</td>\n",
       "      <td>1307</td>\n",
       "      <td>0.919775</td>\n",
       "      <td>0.926293</td>\n",
       "      <td>0.923023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRO</th>\n",
       "      <td>80</td>\n",
       "      <td>85</td>\n",
       "      <td>76</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.921212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RACE</th>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TITLE</th>\n",
       "      <td>1889</td>\n",
       "      <td>1888</td>\n",
       "      <td>1778</td>\n",
       "      <td>0.941239</td>\n",
       "      <td>0.941737</td>\n",
       "      <td>0.941488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>4083</td>\n",
       "      <td>4069</td>\n",
       "      <td>3828</td>\n",
       "      <td>0.937546</td>\n",
       "      <td>0.940772</td>\n",
       "      <td>0.939156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Anno  Pred  Match         R         P        F1\n",
       "CONT     90    88     88  0.977778  1.000000  0.988764\n",
       "EDU     263   267    250  0.950570  0.936330  0.943396\n",
       "LOC       9     8      8  0.888889  1.000000  0.941176\n",
       "NAME    302   295    294  0.973510  0.996610  0.984925\n",
       "ORG    1421  1411   1307  0.919775  0.926293  0.923023\n",
       "PRO      80    85     76  0.950000  0.894118  0.921212\n",
       "RACE     29    27     27  0.931034  1.000000  0.964286\n",
       "TITLE  1889  1888   1778  0.941239  0.941737  0.941488\n",
       "E      4083  4069   3828  0.937546  0.940772  0.939156"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crfpp.train import train\n",
    "\n",
    "Performance = train(model, sents, Channel_Settings, labels, cross_num, cross_validation = None, seed = 10)\n",
    "\n",
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T03:57:00.044577Z",
     "start_time": "2019-10-03T03:56:59.974083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('（', '标点-B'),\n",
       " ('6', '数字-B'),\n",
       " ('）', '标点-B'),\n",
       " ('不', '无-B'),\n",
       " ('能', '有-B'),\n",
       " ('规', '定性-B'),\n",
       " ('律', '定性-I'),\n",
       " ('进', '事件-B'),\n",
       " ('食', '事件-I'),\n",
       " ('，', '标点-B'),\n",
       " ('或', '连词-B'),\n",
       " ('不', '无-B'),\n",
       " ('能', '有-B'),\n",
       " ('配', '事件-B'),\n",
       " ('合', '事件-I'),\n",
       " ('饮', '身体功能-B'),\n",
       " ('食', '身体功能-I'),\n",
       " ('干', '定性-B'),\n",
       " ('预', '定性-I'),\n",
       " ('者', '代词-B')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crfpp.tagger import tagger\n",
    "from nlptext.sentence import Sentence\n",
    "\n",
    "model = 'model/MedPos/char/T_b-n1t1-f1_m-n1t1-f1_r-n1t1-f1_P-bio_E-bio'\n",
    "sent_str = '（6）不能规律进食，或不能配合饮食干预者'\n",
    "\n",
    "sent = Sentence(sentence=[i for i in sent_str.replace(' ', '@')])\n",
    "\n",
    "TOKENLevel = 'word' if 'word' in model else 'char'\n",
    "sent.TOKEN['TOKENLevel'] = TOKENLevel\n",
    "\n",
    "result_seq = tagger(model, sent, get_seq = True)\n",
    "list(zip(sent.sentence.split(' '), result_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T03:57:28.372163Z",
     "start_time": "2019-10-03T03:57:28.296638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('（', 0, 1, '标点'),\n",
       " ('6', 1, 2, '数字'),\n",
       " ('）', 2, 3, '标点'),\n",
       " ('不', 3, 4, '无'),\n",
       " ('能', 4, 5, '有'),\n",
       " ('规律', 5, 7, '定性'),\n",
       " ('进食', 7, 9, '事件'),\n",
       " ('，', 9, 10, '标点'),\n",
       " ('或', 10, 11, '连词'),\n",
       " ('不', 11, 12, '无'),\n",
       " ('能', 12, 13, '有'),\n",
       " ('配合', 13, 15, '事件'),\n",
       " ('饮食', 15, 17, '身体功能'),\n",
       " ('干预', 17, 19, '定性'),\n",
       " ('者', 19, 20, '代词')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_entities = tagger(model, sent)\n",
    "sentence = sent.sentence.split(' ')\n",
    "\n",
    "L = []\n",
    "for i in result_entities:\n",
    "    s, e, label = i\n",
    "    L.append((''.join(sentence[s:e]), s, e, label))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
